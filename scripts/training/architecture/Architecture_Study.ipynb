{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY6ZNt1hUNcn",
        "outputId": "64f20985-a23e-4527-a3f5-e6b8a456f705"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üèõÔ∏è STARTING ARCHITECTURE RUN: original_unet\n",
            "================================================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [03:52<00:00,  1.55s/it, loss=0.6161]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Val Loss: 0.640447\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_original_unet.pt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.72it/s, loss=1.0256]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Val Loss: 14.177297\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.79it/s, loss=2.3693]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Val Loss: 2.716541\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.79it/s, loss=1.8890]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Val Loss: 2.593938\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.79it/s, loss=2.0437]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Val Loss: 5.336793\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.80it/s, loss=3.2288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val Loss: 6.151578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.81it/s, loss=3.8546]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val Loss: 6.281056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25 [original_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.80it/s, loss=3.4636]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val Loss: 8.409728\n",
            "   üõë Early stopping triggered.\n",
            "üèõÔ∏è FINISHED ARCHITECTURE RUN: original_unet | Best Val Loss: 0.640447\n",
            "\n",
            "================================================================================\n",
            "üèõÔ∏è STARTING ARCHITECTURE RUN: resnet_unet\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:35<00:00,  1.58it/s, loss=0.7202]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Loss: 0.815562\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_resnet_unet.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:15<00:00,  1.98it/s, loss=1.0095]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Loss: 1.556403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:15<00:00,  1.98it/s, loss=1.9695]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Loss: 2.878543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:15<00:00,  1.98it/s, loss=1.2798]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Loss: 2.071224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:15<00:00,  1.98it/s, loss=1.1761]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Val Loss: 1.065803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:15<00:00,  1.98it/s, loss=0.7664]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val Loss: 0.894059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:15<00:00,  1.98it/s, loss=1.3900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val Loss: 1.285973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25 [resnet_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:15<00:00,  1.98it/s, loss=1.0887]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val Loss: 1.120279\n",
            "   üõë Early stopping triggered.\n",
            "üèõÔ∏è FINISHED ARCHITECTURE RUN: resnet_unet | Best Val Loss: 0.815562\n",
            "\n",
            "================================================================================\n",
            "üèõÔ∏è STARTING ARCHITECTURE RUN: attention_unet\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:48<00:00,  3.08it/s, loss=0.5930]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Loss: 57.362974\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_attention_unet.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.76it/s, loss=0.9696]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Loss: 1.586533\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_attention_unet.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:32<00:00,  4.66it/s, loss=1.5181]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Loss: 4.686310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.80it/s, loss=1.7635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Loss: 4.739056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.79it/s, loss=2.0737]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Val Loss: 3.381161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.80it/s, loss=2.2166]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val Loss: 7.329491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.81it/s, loss=2.3247]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val Loss: 1.960474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.80it/s, loss=2.0185]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val Loss: 1.812630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/25 [attention_unet]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:31<00:00,  4.80it/s, loss=2.4235]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Val Loss: 1.949027\n",
            "   üõë Early stopping triggered.\n",
            "üèõÔ∏è FINISHED ARCHITECTURE RUN: attention_unet | Best Val Loss: 1.586533\n",
            "\n",
            "================================================================================\n",
            "üèõÔ∏è STARTING ARCHITECTURE RUN: lightweight_cnn\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:23<00:00,  6.32it/s, loss=0.6553]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Loss: 0.645011\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_lightweight_cnn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.06it/s, loss=0.7888]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Loss: 0.629705\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_lightweight_cnn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 19.97it/s, loss=0.5621]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Loss: 0.634293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.52it/s, loss=0.4842]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Loss: 0.631113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 19.84it/s, loss=0.6128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Val Loss: 0.614137\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_lightweight_cnn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.47it/s, loss=0.6296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val Loss: 0.618693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 19.73it/s, loss=0.5259]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val Loss: 0.629660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.00it/s, loss=0.9393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val Loss: 0.616834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.19it/s, loss=1.0355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Val Loss: 0.629355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 21.00it/s, loss=0.7389]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Val Loss: 0.612613\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_lightweight_cnn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 19.97it/s, loss=0.6777]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Val Loss: 0.606358\n",
            "   ‚úÖ Best model saved to /content/drive/My Drive/AR_Downscaling/architecture_study_models/arch_lightweight_cnn.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.23it/s, loss=0.8178]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Val Loss: 0.609687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.01it/s, loss=0.4720]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Val Loss: 0.630632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.18it/s, loss=0.6124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Val Loss: 0.607495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.11it/s, loss=0.6216]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Val Loss: 0.606666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 19.94it/s, loss=0.6403]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Val Loss: 0.610036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.73it/s, loss=0.6624]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Val Loss: 0.627995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/25 [lightweight_cnn]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 20.09it/s, loss=0.4933]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Val Loss: 0.608713\n",
            "   üõë Early stopping triggered.\n",
            "üèõÔ∏è FINISHED ARCHITECTURE RUN: lightweight_cnn | Best Val Loss: 0.606358\n",
            "\n",
            "\n",
            "üéâüéâüéâ All architecture models trained successfully! üéâüéâüéâ\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# This script systematically trains different model architectures as regression models\n",
        "# to conduct a scientifically rigorous architectural generalization study.\n",
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import gc\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "OUTPUT_DIR = PROJECT_PATH / 'architecture_study_models'\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Training Hyperparameters (MUST remain constant for all models) ---\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 1e-4\n",
        "EARLY_STOPPING_PATIENCE = 7\n",
        "GRADIENT_CLIP = 1.0\n",
        "INPUT_CHANNELS = 5 # All models will use the full 5-variable input\n",
        "\n",
        "# --- 2. DATASET & LOSS FUNCTION ---\n",
        "class MultiVariableARDataset(Dataset):\n",
        "    def __init__(self, data_dir: Path, split: str = 'train'):\n",
        "        self.split_dir = data_dir / split\n",
        "        self.predictor_files = sorted(list(self.split_dir.glob('*_predictor.npy')))\n",
        "        self.stats = joblib.load(data_dir / 'normalization_stats_multi_variable.joblib')\n",
        "        if not self.predictor_files: raise FileNotFoundError(f\"No predictor files in {self.split_dir}\")\n",
        "\n",
        "    def __len__(self): return len(self.predictor_files)\n",
        "    def __getitem__(self, idx):\n",
        "        pred_path = self.predictor_files[idx]\n",
        "        targ_path = Path(str(pred_path).replace('_predictor.npy', '_target.npy'))\n",
        "        predictor_data = np.load(pred_path).astype(np.float32)\n",
        "        target_data = np.load(targ_path).astype(np.float32)\n",
        "        predictor_norm = (predictor_data - self.stats['predictor_mean'][:, None, None]) / (self.stats['predictor_std'][:, None, None] + 1e-8)\n",
        "        target_norm = (target_data - self.stats['target_mean']) / (self.stats['target_std'] + 1e-8)\n",
        "        return torch.from_numpy(predictor_norm), torch.from_numpy(target_norm).unsqueeze(0)\n",
        "\n",
        "class AdvancedLoss(nn.Module):\n",
        "    def __init__(self, mse_weight=0.6, ssim_weight=0.2, gradient_weight=0.2):\n",
        "        super().__init__()\n",
        "        self.mse_weight, self.ssim_weight, self.gradient_weight = mse_weight, ssim_weight, gradient_weight\n",
        "        self.mse_loss, self.l1_loss = nn.MSELoss(), nn.L1Loss()\n",
        "    def ssim_loss(self, pred, target, window_size=11):\n",
        "        mu1 = F.avg_pool2d(pred, window_size, 1, padding=window_size//2)\n",
        "        mu2 = F.avg_pool2d(target, window_size, 1, padding=window_size//2)\n",
        "        mu1_sq, mu2_sq, mu1_mu2 = mu1**2, mu2**2, mu1 * mu2\n",
        "        sigma1_sq = F.avg_pool2d(pred**2, window_size, 1, padding=window_size//2) - mu1_sq\n",
        "        sigma2_sq = F.avg_pool2d(target**2, window_size, 1, padding=window_size//2) - mu2_sq\n",
        "        sigma12 = F.avg_pool2d(pred*target, window_size, 1, padding=window_size//2) - mu1_mu2\n",
        "        c1, c2 = 0.01**2, 0.03**2\n",
        "        ssim_map = ((2*mu1_mu2 + c1) * (2*sigma12 + c2)) / ((mu1_sq + mu2_sq + c1) * (sigma1_sq + sigma2_sq + c2))\n",
        "        return 1 - ssim_map.mean()\n",
        "    def gradient_loss(self, pred, target):\n",
        "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "        pred_grad_x, pred_grad_y = F.conv2d(pred, sobel_x, padding=1), F.conv2d(pred, sobel_y, padding=1)\n",
        "        target_grad_x, target_grad_y = F.conv2d(target, sobel_x, padding=1), F.conv2d(target, sobel_y, padding=1)\n",
        "        return self.l1_loss(pred_grad_x, target_grad_x) + self.l1_loss(pred_grad_y, target_grad_y)\n",
        "    def forward(self, pred, target):\n",
        "        mse = self.mse_loss(pred, target)\n",
        "        ssim = self.ssim_loss(pred, target)\n",
        "        grad = self.gradient_loss(pred, target)\n",
        "        return (self.mse_weight * mse + self.ssim_weight * ssim + self.gradient_weight * grad)\n",
        "\n",
        "# --- 3. MODEL ARCHITECTURES ---\n",
        "# (Note: Final activation is NOT Tanh for regression models)\n",
        "class SimplifiedAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channel_att = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(channels, channels // 16, 1), nn.ReLU(inplace=True), nn.Conv2d(channels // 16, channels, 1), nn.Sigmoid())\n",
        "        self.spatial_att = nn.Sequential(nn.Conv2d(channels, 1, 7, padding=3), nn.Sigmoid())\n",
        "    def forward(self, x):\n",
        "        ch_att = self.channel_att(x); x = x * ch_att; sp_att = self.spatial_att(x); x = x * sp_att\n",
        "        return x\n",
        "\n",
        "class OriginalUNet(nn.Module): # This is your MultiVarAdvancedDownscaler\n",
        "    def __init__(self, input_channels=5, base_channels=64, depth=4, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.use_attention, self.depth = use_attention, depth\n",
        "        self.channels = [base_channels * min(2**i, 8) for i in range(depth)]\n",
        "        self.encoders, self.downsamplers = nn.ModuleList(), nn.ModuleList()\n",
        "        in_ch = input_channels\n",
        "        for i, out_ch in enumerate(self.channels):\n",
        "            self.encoders.append(self._conv_block(in_ch, out_ch))\n",
        "            if i < len(self.channels) - 1: self.downsamplers.append(nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1))\n",
        "            in_ch = out_ch\n",
        "        bottleneck_ch = self.channels[-1]\n",
        "        self.bottleneck = self._conv_block(self.channels[-1], bottleneck_ch)\n",
        "        if self.use_attention: self.attention = SimplifiedAttention(bottleneck_ch)\n",
        "        self.upsamplers, self.decoders = nn.ModuleList(), nn.ModuleList()\n",
        "        for i in range(depth - 1, -1, -1):\n",
        "            in_ch = bottleneck_ch if i == depth - 1 else self.channels[i+1]\n",
        "            out_ch = self.channels[i]\n",
        "            self.upsamplers.append(nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2))\n",
        "            self.decoders.append(self._conv_block(out_ch * 2, out_ch))\n",
        "        self.final_conv = nn.Sequential(nn.Conv2d(self.channels[0], self.channels[0] // 2, 3, padding=1), nn.BatchNorm2d(self.channels[0] // 2), nn.ReLU(inplace=True), nn.Conv2d(self.channels[0] // 2, 1, 1))\n",
        "    def _conv_block(self, in_ch, out_ch): return nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        for i in range(len(self.encoders)):\n",
        "            x = self.encoders[i](x); skips.append(x)\n",
        "            if i < len(self.downsamplers): x = self.downsamplers[i](x)\n",
        "        x = self.bottleneck(x)\n",
        "        if self.use_attention: x = self.attention(x)\n",
        "        for i, (up, dec) in enumerate(zip(self.upsamplers, self.decoders)):\n",
        "            x = up(x); skip = skips[len(skips) - 1 - i]\n",
        "            if x.shape[-2:] != skip.shape[-2:]: x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            x = torch.cat([x, skip], dim=1); x = dec(x)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self, x):\n",
        "        identity = x; out = self.relu(self.bn1(self.conv1(x))); out = self.bn2(self.conv2(out)); out += identity\n",
        "        return self.relu(out)\n",
        "\n",
        "class ResNetUNet(nn.Module):\n",
        "    def __init__(self, input_channels=5, base_channels=64, depth=4):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        channels = [base_channels * min(2**i, 8) for i in range(depth)]\n",
        "        self.initial_conv = nn.Conv2d(input_channels, channels[0], 3, padding=1)\n",
        "        self.encoders, self.downsamplers = nn.ModuleList(), nn.ModuleList()\n",
        "        for i, ch in enumerate(channels):\n",
        "            self.encoders.append(nn.Sequential(ResNetBlock(ch), ResNetBlock(ch)))\n",
        "            if i < len(channels) - 1: self.downsamplers.append(nn.Conv2d(ch, channels[i+1], 3, stride=2, padding=1))\n",
        "        self.bottleneck = nn.Sequential(ResNetBlock(channels[-1]), ResNetBlock(channels[-1]), ResNetBlock(channels[-1]))\n",
        "        self.upsamplers, self.decoders = nn.ModuleList(), nn.ModuleList()\n",
        "        for i in range(depth-1, 0, -1):\n",
        "            self.upsamplers.append(nn.ConvTranspose2d(channels[i], channels[i-1], 2, stride=2))\n",
        "            self.decoders.append(nn.Sequential(ResNetBlock(channels[i-1] * 2), ResNetBlock(channels[i-1] * 2), nn.Conv2d(channels[i-1] * 2, channels[i-1], 1)))\n",
        "        self.final_conv = nn.Sequential(nn.Conv2d(channels[0], channels[0]//2, 3, padding=1), nn.BatchNorm2d(channels[0]//2), nn.ReLU(inplace=True), nn.Conv2d(channels[0]//2, 1, 1))\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x); skips = []\n",
        "        for i, (encoder, downsampler) in enumerate(zip(self.encoders, self.downsamplers)):\n",
        "            x = encoder(x); skips.append(x); x = downsampler(x)\n",
        "        x = self.encoders[-1](x); skips.append(x); x = self.bottleneck(x)\n",
        "        for i, (upsampler, decoder) in enumerate(zip(self.upsamplers, self.decoders)):\n",
        "            x = upsampler(x); skip = skips[len(skips) - 2 - i]\n",
        "            if x.shape[-2:] != skip.shape[-2:]: x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            x = torch.cat([x, skip], dim=1); x = decoder(x)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class LightweightCNN(nn.Module):\n",
        "    def __init__(self, input_channels=5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, 7, padding=3), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 5, padding=2), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 1, 3, padding=1) # No Tanh\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        original_size = x.shape[-2:]\n",
        "        out = self.features(x)\n",
        "        if out.shape[-2:] != original_size:\n",
        "            out = F.interpolate(out, size=original_size, mode='bilinear', align_corners=False)\n",
        "        return out\n",
        "\n",
        "# --- 4. ARCHITECTURE CONFIGURATION ---\n",
        "ARCHITECTURE_CONFIGS = {\n",
        "    'original_unet': {'class': OriginalUNet, 'params': {'input_channels': INPUT_CHANNELS}},\n",
        "    'resnet_unet': {'class': ResNetUNet, 'params': {'input_channels': INPUT_CHANNELS}},\n",
        "    'attention_unet': {'class': OriginalUNet, 'params': {'input_channels': INPUT_CHANNELS, 'use_attention': True}}, # Your original is an attention unet\n",
        "    'lightweight_cnn': {'class': LightweightCNN, 'params': {'input_channels': INPUT_CHANNELS}},\n",
        "}\n",
        "\n",
        "# --- 5. CONSISTENT TRAINING FUNCTION ---\n",
        "def train_one_architecture(config):\n",
        "    arch_name = config['name']\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"üèõÔ∏è STARTING ARCHITECTURE RUN: {arch_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_dataset = MultiVariableARDataset(DATA_DIR, 'train')\n",
        "    val_dataset = MultiVariableARDataset(DATA_DIR, 'val')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = config['class'](**config['params']).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = AdvancedLoss()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [{arch_name}]\")\n",
        "        for predictor, target in pbar:\n",
        "            predictor, target = predictor.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                prediction = model(predictor)\n",
        "                loss = loss_fn(prediction, target)\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for predictor, target in val_loader:\n",
        "                predictor, target = predictor.to(device), target.to(device)\n",
        "                prediction = model(predictor)\n",
        "                val_loss += loss_fn(prediction, target).item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            model_path = OUTPUT_DIR / f\"arch_{arch_name}.pt\"\n",
        "            metadata_path = OUTPUT_DIR / f\"arch_{arch_name}.json\"\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            with open(metadata_path, 'w') as f:\n",
        "                # Save info needed to reload the model later\n",
        "                json.dump({\n",
        "                    'name': arch_name,\n",
        "                    'class': model.__class__.__name__,\n",
        "                    'params': config['params']\n",
        "                }, f, indent=2)\n",
        "            print(f\"   ‚úÖ Best model saved to {model_path}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"   üõë Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    print(f\"üèõÔ∏è FINISHED ARCHITECTURE RUN: {arch_name} | Best Val Loss: {best_val_loss:.6f}\")\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- 6. MAIN EXECUTION SCRIPT ---\n",
        "def main():\n",
        "    for name, config in ARCHITECTURE_CONFIGS.items():\n",
        "        config['name'] = name\n",
        "        train_one_architecture(config)\n",
        "\n",
        "    print(\"\\n\\nüéâüéâüéâ All architecture models trained successfully! üéâüéâüéâ\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# This script performs a comprehensive, quantitative evaluation comparing the\n",
        "# results of the architectural generalization study.\n",
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.ndimage import sobel\n",
        "from torchvision.transforms import Resize\n",
        "import json\n",
        "import math\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "MODEL_DIR = PROJECT_PATH / 'architecture_study_models' # Directory where arch models were saved\n",
        "OUTPUT_DIR = PROJECT_PATH / 'final_evaluation_results'\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Evaluation Configuration ---\n",
        "CSI_THRESHOLDS_K = [230.0, 220.0, 210.0]\n",
        "IMG_SIZE = 256 # All these models were trained on 256x256\n",
        "\n",
        "# --- 2. DATASET & MODEL ARCHITECTURES ---\n",
        "class MultiVariableARDataset(Dataset):\n",
        "    def __init__(self, data_dir: Path, split: str = 'test'):\n",
        "        self.split_dir = data_dir / split\n",
        "        self.predictor_files = sorted(list(self.split_dir.glob('*_predictor.npy')))\n",
        "        self.stats = joblib.load(data_dir / 'normalization_stats_multi_variable.joblib')\n",
        "        if not self.predictor_files: raise FileNotFoundError(f\"No predictor files in {self.split_dir}\")\n",
        "        print(f\"Loaded '{split}' dataset with {len(self.predictor_files)} samples.\")\n",
        "\n",
        "    def __len__(self): return len(self.predictor_files)\n",
        "    def __getitem__(self, idx):\n",
        "        pred_path = self.predictor_files[idx]\n",
        "        targ_path = Path(str(pred_path).replace('_predictor.npy', '_target.npy'))\n",
        "        predictor_data = np.load(pred_path).astype(np.float32)\n",
        "        target_data = np.load(targ_path).astype(np.float32)\n",
        "        predictor_norm = (predictor_data - self.stats['predictor_mean'][:, None, None]) / (self.stats['predictor_std'][:, None, None] + 1e-8)\n",
        "        target_norm = (target_data - self.stats['target_mean']) / (self.stats['target_std'] + 1e-8)\n",
        "        return torch.from_numpy(predictor_norm), torch.from_numpy(target_norm).unsqueeze(0)\n",
        "\n",
        "class SimplifiedAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channel_att = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(channels, channels // 16, 1), nn.ReLU(inplace=True), nn.Conv2d(channels // 16, channels, 1), nn.Sigmoid())\n",
        "        self.spatial_att = nn.Sequential(nn.Conv2d(channels, 1, 7, padding=3), nn.Sigmoid())\n",
        "    def forward(self, x):\n",
        "        ch_att = self.channel_att(x); x = x * ch_att; sp_att = self.spatial_att(x); x = x * sp_att\n",
        "        return x\n",
        "\n",
        "class OriginalUNet(nn.Module):\n",
        "    def __init__(self, input_channels=5, base_channels=64, depth=4, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.use_attention, self.depth = use_attention, depth\n",
        "        self.channels = [base_channels * min(2**i, 8) for i in range(depth)]\n",
        "        self.encoders, self.downsamplers = nn.ModuleList(), nn.ModuleList()\n",
        "        in_ch = input_channels\n",
        "        for i, out_ch in enumerate(self.channels):\n",
        "            self.encoders.append(self._conv_block(in_ch, out_ch))\n",
        "            if i < len(self.channels) - 1: self.downsamplers.append(nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1))\n",
        "            in_ch = out_ch\n",
        "        bottleneck_ch = self.channels[-1]\n",
        "        self.bottleneck = self._conv_block(self.channels[-1], bottleneck_ch)\n",
        "        if self.use_attention: self.attention = SimplifiedAttention(bottleneck_ch)\n",
        "        self.upsamplers, self.decoders = nn.ModuleList(), nn.ModuleList()\n",
        "        for i in range(depth - 1, -1, -1):\n",
        "            in_ch = bottleneck_ch if i == depth - 1 else self.channels[i+1]\n",
        "            out_ch = self.channels[i]\n",
        "            self.upsamplers.append(nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2))\n",
        "            self.decoders.append(self._conv_block(out_ch * 2, out_ch))\n",
        "        self.final_conv = nn.Sequential(nn.Conv2d(self.channels[0], self.channels[0] // 2, 3, padding=1), nn.BatchNorm2d(self.channels[0] // 2), nn.ReLU(inplace=True), nn.Conv2d(self.channels[0] // 2, 1, 1))\n",
        "    def _conv_block(self, in_ch, out_ch): return nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        for i in range(len(self.encoders)):\n",
        "            x = self.encoders[i](x); skips.append(x)\n",
        "            if i < len(self.downsamplers): x = self.downsamplers[i](x)\n",
        "        x = self.bottleneck(x)\n",
        "        if self.use_attention: x = self.attention(x)\n",
        "        for i, (up, dec) in enumerate(zip(self.upsamplers, self.decoders)):\n",
        "            x = up(x); skip = skips[len(skips) - 1 - i]\n",
        "            if x.shape[-2:] != skip.shape[-2:]: x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            x = torch.cat([x, skip], dim=1); x = dec(x)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self, x):\n",
        "        identity = x; out = self.relu(self.bn1(self.conv1(x))); out = self.bn2(self.conv2(out)); out += identity\n",
        "        return self.relu(out)\n",
        "\n",
        "class ResNetUNet(nn.Module):\n",
        "    def __init__(self, input_channels=5, base_channels=64, depth=4):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        channels = [base_channels * min(2**i, 8) for i in range(depth)]\n",
        "        self.initial_conv = nn.Conv2d(input_channels, channels[0], 3, padding=1)\n",
        "        self.encoders, self.downsamplers = nn.ModuleList(), nn.ModuleList()\n",
        "        for i, ch in enumerate(channels):\n",
        "            self.encoders.append(nn.Sequential(ResNetBlock(ch), ResNetBlock(ch)))\n",
        "            if i < len(channels) - 1: self.downsamplers.append(nn.Conv2d(ch, channels[i+1], 3, stride=2, padding=1))\n",
        "        self.bottleneck = nn.Sequential(ResNetBlock(channels[-1]), ResNetBlock(channels[-1]), ResNetBlock(channels[-1]))\n",
        "        self.upsamplers, self.decoders = nn.ModuleList(), nn.ModuleList()\n",
        "        for i in range(depth-1, 0, -1):\n",
        "            self.upsamplers.append(nn.ConvTranspose2d(channels[i], channels[i-1], 2, stride=2))\n",
        "            self.decoders.append(nn.Sequential(ResNetBlock(channels[i-1] * 2), ResNetBlock(channels[i-1] * 2), nn.Conv2d(channels[i-1] * 2, channels[i-1], 1)))\n",
        "        self.final_conv = nn.Sequential(nn.Conv2d(channels[0], channels[0]//2, 3, padding=1), nn.BatchNorm2d(channels[0]//2), nn.ReLU(inplace=True), nn.Conv2d(channels[0]//2, 1, 1))\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x); skips = []\n",
        "        for i, (encoder, downsampler) in enumerate(zip(self.encoders, self.downsamplers)):\n",
        "            x = encoder(x); skips.append(x); x = downsampler(x)\n",
        "        x = self.encoders[-1](x); skips.append(x); x = self.bottleneck(x)\n",
        "        for i, (upsampler, decoder) in enumerate(zip(self.upsamplers, self.decoders)):\n",
        "            x = upsampler(x); skip = skips[len(skips) - 2 - i]\n",
        "            if x.shape[-2:] != skip.shape[-2:]: x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            x = torch.cat([x, skip], dim=1); x = decoder(x)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class LightweightCNN(nn.Module):\n",
        "    def __init__(self, input_channels=5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, 7, padding=3), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 5, padding=2), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 1, 3, padding=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        original_size = x.shape[-2:]\n",
        "        out = self.features(x)\n",
        "        if out.shape[-2:] != original_size:\n",
        "            out = F.interpolate(out, size=original_size, mode='bilinear', align_corners=False)\n",
        "        return out\n",
        "\n",
        "# --- 3. METRIC & MODEL LOADING FUNCTIONS ---\n",
        "def calculate_detailed_csi(pred_k, true_k, threshold_k):\n",
        "    pred_mask = pred_k <= threshold_k; true_mask = true_k <= threshold_k\n",
        "    hits = (pred_mask & true_mask).sum()\n",
        "    misses = (~pred_mask & true_mask).sum()\n",
        "    false_alarms = (pred_mask & ~true_mask).sum()\n",
        "    csi = hits / (hits + misses + false_alarms) if (hits + misses + false_alarms) > 0 else 0.0\n",
        "    event_frequency = true_mask.sum() / true_mask.size\n",
        "    return csi, event_frequency\n",
        "\n",
        "def calculate_gradient_magnitude(image):\n",
        "    grad_x = sobel(image, axis=0); grad_y = sobel(image, axis=1)\n",
        "    return np.sqrt(grad_x**2 + grad_y**2).mean()\n",
        "\n",
        "def calculate_all_metrics(pred_norm, true_norm, stats):\n",
        "    pred_k = pred_norm * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "    true_k = true_norm * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "    metrics = {\n",
        "        'rmse': np.sqrt(np.mean((pred_k - true_k)**2)),\n",
        "        'mae': np.mean(np.abs(pred_k - true_k)),\n",
        "        'sharpness': calculate_gradient_magnitude(pred_k),\n",
        "        'distribution_dist': wasserstein_distance(pred_k.flatten(), true_k.flatten())\n",
        "    }\n",
        "    for thr in CSI_THRESHOLDS_K:\n",
        "        csi, freq = calculate_detailed_csi(pred_k, true_k, thr)\n",
        "        metrics[f'csi_{int(thr)}K'] = csi\n",
        "        metrics[f'freq_{int(thr)}K'] = freq\n",
        "    return metrics\n",
        "\n",
        "def get_model_class_from_name(class_name):\n",
        "    \"\"\"Helper to get class object from its string name.\"\"\"\n",
        "    if class_name == 'OriginalUNet': return OriginalUNet\n",
        "    if class_name == 'ResNetUNet': return ResNetUNet\n",
        "    if class_name == 'LightweightCNN': return LightweightCNN\n",
        "    raise ValueError(f\"Unknown model class name: {class_name}\")\n",
        "\n",
        "# --- 4. MAIN EVALUATION SCRIPT ---\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"üîß Starting evaluation on device: {device}\")\n",
        "\n",
        "    test_dataset = MultiVariableARDataset(DATA_DIR, 'test')\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "    stats = test_dataset.stats\n",
        "\n",
        "    # --- Find and Load Models Automatically ---\n",
        "    metadata_files = sorted(list(MODEL_DIR.glob('arch_*.json')))\n",
        "    if not metadata_files:\n",
        "        print(f\"‚ùå No model metadata files found in {MODEL_DIR}. Cannot evaluate.\")\n",
        "        return\n",
        "\n",
        "    loaded_models = {}\n",
        "    for meta_file in metadata_files:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        name = metadata['name']\n",
        "        model_class_name = metadata['class']\n",
        "        params = metadata['params']\n",
        "        model_path = meta_file.with_suffix('.pt')\n",
        "\n",
        "        if not model_path.exists():\n",
        "            print(f\"‚ö†Ô∏è Model file not found for '{name}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Loading model: '{name}' (Class: {model_class_name})\")\n",
        "        ModelClass = get_model_class_from_name(model_class_name)\n",
        "        model = ModelClass(**params).to(device)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval()\n",
        "        loaded_models[name] = model\n",
        "\n",
        "    # --- Evaluate Each Model ---\n",
        "    all_results = []\n",
        "    for name, model in loaded_models.items():\n",
        "        print(f\"\\n--- Evaluating {name} ---\")\n",
        "        model_metrics = []\n",
        "        with torch.no_grad():\n",
        "            for predictor, target in tqdm(test_loader, desc=f\"Predicting with {name}\"):\n",
        "                predictor = predictor.to(device)\n",
        "                pred_norm = model(predictor).cpu().numpy().squeeze()\n",
        "                target_norm = target.numpy().squeeze()\n",
        "                metrics = calculate_all_metrics(pred_norm, target_norm, stats)\n",
        "                model_metrics.append(metrics)\n",
        "\n",
        "        df = pd.DataFrame(model_metrics)\n",
        "        summary = {\"Model\": name}\n",
        "        for col in df.columns:\n",
        "            summary[f\"{col}_mean\"] = df[col].mean()\n",
        "            summary[f\"{col}_std\"] = df[col].std()\n",
        "        all_results.append(summary)\n",
        "\n",
        "    if not all_results:\n",
        "        print(\"‚ùå No models were evaluated. Exiting.\")\n",
        "        return\n",
        "\n",
        "    results_df = pd.DataFrame(all_results).set_index(\"Model\")\n",
        "\n",
        "    # --- Reporting ---\n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"üèõÔ∏è FINAL ARCHITECTURE EVALUATION REPORT üèõÔ∏è\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n--- I. Overall Performance Metrics ---\")\n",
        "    std_metrics_df = results_df[['rmse_mean', 'mae_mean', 'sharpness_mean', 'distribution_dist_mean']]\n",
        "    print(std_metrics_df.round(4))\n",
        "\n",
        "    print(\"\\n--- II. Critical Success Index (CSI) by Threshold ---\")\n",
        "\n",
        "    # --- FIX: Explicitly select and order the '_mean' columns for the report ---\n",
        "    csi_cols_ordered = []\n",
        "    pretty_cols = []\n",
        "    for thr in CSI_THRESHOLDS_K:\n",
        "        csi_cols_ordered.append(f'csi_{int(thr)}K_mean')\n",
        "        csi_cols_ordered.append(f'freq_{int(thr)}K_mean')\n",
        "        pretty_cols.append((f\"T <= {int(thr)}K\", \"CSI\"))\n",
        "        pretty_cols.append((f\"T <= {int(thr)}K\", \"Event Freq.\"))\n",
        "\n",
        "    csi_df = results_df[csi_cols_ordered]\n",
        "    csi_df.columns = pd.MultiIndex.from_tuples(pretty_cols)\n",
        "    csi_df = csi_df.sort_index(axis=1)\n",
        "\n",
        "    print(csi_df.round(4))\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    results_df.to_csv(OUTPUT_DIR / 'architecture_study_evaluation.csv')\n",
        "    print(f\"\\nüíæ Detailed results saved to: {OUTPUT_DIR / 'architecture_study_evaluation.csv'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wgls7myb0Dl",
        "outputId": "c9357965-35ba-4176-a3ae-498a3921404e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Starting evaluation on device: cuda\n",
            "Loaded 'test' dataset with 150 samples.\n",
            "Loading model: 'attention_unet' (Class: OriginalUNet)\n",
            "Loading model: 'lightweight_cnn' (Class: LightweightCNN)\n",
            "Loading model: 'original_unet' (Class: OriginalUNet)\n",
            "Loading model: 'resnet_unet' (Class: ResNetUNet)\n",
            "\n",
            "--- Evaluating attention_unet ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting with attention_unet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:08<00:00, 16.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating lightweight_cnn ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting with lightweight_cnn: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:07<00:00, 19.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating original_unet ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting with original_unet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:08<00:00, 16.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating resnet_unet ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting with resnet_unet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:10<00:00, 13.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "================================================================================\n",
            "üèõÔ∏è FINAL ARCHITECTURE EVALUATION REPORT üèõÔ∏è\n",
            "================================================================================\n",
            "\n",
            "--- I. Overall Performance Metrics ---\n",
            "                 rmse_mean  mae_mean  sharpness_mean  distribution_dist_mean\n",
            "Model                                                                       \n",
            "attention_unet     15.0333   13.0326          2.4606                 11.6406\n",
            "lightweight_cnn     8.5555    7.2118          0.9943                  5.8837\n",
            "original_unet       8.7593    7.4865          1.1162                  6.3254\n",
            "resnet_unet        11.1532    9.6379          3.7013                  7.8701\n",
            "\n",
            "--- II. Critical Success Index (CSI) by Threshold ---\n",
            "                T <= 210K             T <= 220K             T <= 230K  \\\n",
            "                      CSI Event Freq.       CSI Event Freq.       CSI   \n",
            "Model                                                                   \n",
            "attention_unet     0.0630      0.0626    0.1979      0.1962    0.4555   \n",
            "lightweight_cnn    0.0037      0.0626    0.1354      0.1962    0.4035   \n",
            "original_unet      0.0107      0.0626    0.1225      0.1962    0.4270   \n",
            "resnet_unet        0.0030      0.0626    0.1544      0.1962    0.4286   \n",
            "\n",
            "                             \n",
            "                Event Freq.  \n",
            "Model                        \n",
            "attention_unet       0.4922  \n",
            "lightweight_cnn      0.4922  \n",
            "original_unet        0.4922  \n",
            "resnet_unet          0.4922  \n",
            "\n",
            "================================================================================\n",
            "\n",
            "üíæ Detailed results saved to: /content/drive/My Drive/AR_Downscaling/final_evaluation_results/architecture_study_evaluation.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}