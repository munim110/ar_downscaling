{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP1_u_3934J4",
        "outputId": "a6de4480-9ae1-42e7-f84f-d879484fbca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "--- Preparing 'test' data ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test files: 100%|██████████| 150/150 [00:04<00:00, 30.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interpolating coarse predictors...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Interpolating samples: 100%|██████████| 150/150 [00:05<00:00, 28.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting spatial neighborhood features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating neighborhood stats: 100%|██████████| 5/5 [00:03<00:00,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading pre-trained RF model from /content/drive/My Drive/AR_Downscaling/publication_experiments/strong_baseline_rf/strong_baseline_rf.joblib...\n",
            "Generating predictions with RF model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.8s\n",
            "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    8.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calculating metrics for each sample in the test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 100%|██████████| 150/150 [00:01<00:00, 117.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Computing final summary statistics...\n",
            "\n",
            "--- FINAL STRONG RF BASELINE RESULTS ---\n",
            "name                : strong_rf_baseline\n",
            "category            : operational_baseline\n",
            "description         : Random Forest with Spatial Neighborhood Features\n",
            "status              : success\n",
            "input_channels      : 5\n",
            "rmse_mean           : 365.6403597660159\n",
            "rmse_std            : 15.540257149241755\n",
            "rmse_median         : 363.37960629116475\n",
            "rmse_min            : 337.79086138189666\n",
            "rmse_max            : 425.8484042652887\n",
            "rmse_count          : 150\n",
            "mae_mean            : 147.4556671229599\n",
            "mae_std             : 39.619337290937516\n",
            "mae_median          : 139.56944490757704\n",
            "mae_min             : 82.1775694017185\n",
            "mae_max             : 282.2787620823265\n",
            "mae_count           : 150\n",
            "r2_mean             : -35.10850304799859\n",
            "r2_std              : 105.74245078799508\n",
            "r2_median           : -3.5071413276293097\n",
            "r2_min              : -964.1057640718251\n",
            "r2_max              : -0.05446853222253725\n",
            "r2_count            : 150\n",
            "correlation_mean    : 0.1107572475655372\n",
            "correlation_std     : 0.09216022869794528\n",
            "correlation_median  : 0.10618376438585302\n",
            "correlation_min     : -0.07391866249703391\n",
            "correlation_max     : 0.44481107671328624\n",
            "correlation_count   : 150\n",
            "ssim_mean           : 0.920775445205594\n",
            "ssim_std            : 0.021410110866233296\n",
            "ssim_median         : 0.9209902864915893\n",
            "ssim_min            : 0.8646537843231962\n",
            "ssim_max            : 0.965559962576362\n",
            "ssim_count          : 150\n",
            "csi_mean            : 0.0\n",
            "csi_std             : 0.0\n",
            "csi_median          : 0.0\n",
            "csi_min             : 0.0\n",
            "csi_max             : 0.0\n",
            "csi_count           : 150\n",
            "fss_mean            : 1.0\n",
            "fss_std             : 0.0\n",
            "fss_median          : 1.0\n",
            "fss_min             : 1.0\n",
            "fss_max             : 1.0\n",
            "fss_count           : 150\n",
            "\n",
            "✅ Results saved to /content/drive/My Drive/AR_Downscaling/publication_experiments/strong_baseline_rf/strong_rf_final_detailed_results.csv\n"
          ]
        }
      ],
      "source": [
        "# --- STANDALONE EVALUATION SCRIPT FOR STRONG RF BASELINE ---\n",
        "# This script loads a pre-trained Random Forest model and evaluates it on the test set,\n",
        "# calculating the full suite of summary statistics as requested.\n",
        "# Includes robust data loading to handle inconsistent image dimensions.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from scipy.ndimage import zoom, uniform_filter\n",
        "import warnings\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "except:\n",
        "    PROJECT_PATH = Path('.') # For local execution\n",
        "\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "MODEL_DIR = PROJECT_PATH / 'publication_experiments' / 'strong_baseline_rf'\n",
        "OUTPUT_DIR = MODEL_DIR\n",
        "\n",
        "TARGET_SHAPE = (256, 256)\n",
        "STATS = joblib.load(DATA_DIR / 'normalization_stats_multi_variable.joblib')\n",
        "\n",
        "# --- FEATURE ENGINEERING and DATA PREPARATION (Corrected) ---\n",
        "\n",
        "def extract_neighborhood_features(X_interp, window_size=5):\n",
        "    \"\"\"Extracts rich spatial features from the interpolated predictor grid.\"\"\"\n",
        "    print(\"Extracting spatial neighborhood features...\")\n",
        "    num_samples, num_channels, h, w = X_interp.shape\n",
        "    num_features = num_channels * 3\n",
        "    X_pixel_major = X_interp.transpose(0, 2, 3, 1).reshape(num_samples * h * w, num_channels)\n",
        "    features = np.zeros((num_samples * h * w, num_features), dtype=np.float32)\n",
        "    features[:, 0:num_channels] = X_pixel_major\n",
        "\n",
        "    for c in tqdm(range(num_channels), desc=\"Calculating neighborhood stats\"):\n",
        "        local_mean = uniform_filter(X_interp[:, c], size=window_size)\n",
        "        local_sq_mean = uniform_filter(X_interp[:, c]**2, size=window_size)\n",
        "        local_var = local_sq_mean - local_mean**2\n",
        "        local_std = np.sqrt(np.maximum(local_var, 0))\n",
        "        features[:, num_channels + c] = local_mean.flatten()\n",
        "        features[:, (2 * num_channels) + c] = local_std.flatten()\n",
        "\n",
        "    return features\n",
        "\n",
        "def load_and_prepare_data(split='test'):\n",
        "    \"\"\"Loads data, interpolates it, and extracts features for evaluation.\"\"\"\n",
        "    print(f\"\\n--- Preparing '{split}' data ---\")\n",
        "    split_dir = DATA_DIR / split\n",
        "    predictor_files = sorted(list(split_dir.glob('*_predictor.npy')))\n",
        "    num_samples = len(predictor_files)\n",
        "    coarse_shape = np.load(predictor_files[0]).shape[1:]\n",
        "    X_coarse = np.zeros((num_samples, 5, *coarse_shape), dtype=np.float32)\n",
        "    Y_high_res = np.zeros((num_samples, *TARGET_SHAPE), dtype=np.float32)\n",
        "\n",
        "    for i, pred_path in enumerate(tqdm(predictor_files, desc=f\"Loading {split} files\")):\n",
        "        targ_path = Path(str(pred_path).replace('_predictor.npy', '_target.npy'))\n",
        "\n",
        "        predictor_data = np.load(pred_path)\n",
        "        target_data = np.load(targ_path)\n",
        "\n",
        "        # --- ROBUST FIX: Center crop data to the target shape ---\n",
        "        h, w = target_data.shape\n",
        "        th, tw = TARGET_SHAPE\n",
        "        if h != th or w != tw:\n",
        "            # Calculate starting indices for the crop\n",
        "            start_h = max(0, (h - th) // 2)\n",
        "            start_w = max(0, (w - tw) // 2)\n",
        "            # Perform the crop\n",
        "            target_data = target_data[start_h : start_h + th, start_w : start_w + tw]\n",
        "\n",
        "        # Ensure the final shape is correct by padding if necessary (handles smaller images)\n",
        "        if target_data.shape != TARGET_SHAPE:\n",
        "             padded_target = np.zeros(TARGET_SHAPE, dtype=np.float32)\n",
        "             padded_target[:target_data.shape[0], :target_data.shape[1]] = target_data\n",
        "             target_data = padded_target\n",
        "\n",
        "        X_coarse[i] = predictor_data\n",
        "        Y_high_res[i] = target_data\n",
        "\n",
        "    print(\"Interpolating coarse predictors...\")\n",
        "    X_interp = np.zeros((num_samples, 5, *TARGET_SHAPE), dtype=np.float32)\n",
        "    zoom_factors = (TARGET_SHAPE[0] / coarse_shape[0], TARGET_SHAPE[1] / coarse_shape[1])\n",
        "    for i in tqdm(range(num_samples), desc=\"Interpolating samples\"):\n",
        "        for c in range(5):\n",
        "            X_interp[i, c] = zoom(X_coarse[i, c], zoom_factors, order=3)\n",
        "\n",
        "    X_features = extract_neighborhood_features(X_interp)\n",
        "    return X_features, Y_high_res\n",
        "\n",
        "# --- METRIC CALCULATION FUNCTIONS ---\n",
        "\n",
        "def denormalize(data, stats):\n",
        "    return data * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "\n",
        "def calculate_csi(pred, target, threshold=220.0):\n",
        "    pred_event, target_event = (pred <= threshold), (target <= threshold)\n",
        "    hits = np.sum(pred_event & target_event)\n",
        "    misses = np.sum(~pred_event & target_event)\n",
        "    false_alarms = np.sum(pred_event & ~target_event)\n",
        "    return hits / (hits + misses + false_alarms) if (hits + misses + false_alarms) > 0 else 0.0\n",
        "\n",
        "def calculate_fss(pred, target, threshold=220.0, window_size=11):\n",
        "    pred_binary, target_binary = (pred <= threshold).astype(float), (target <= threshold).astype(float)\n",
        "    pred_fractions = uniform_filter(pred_binary, size=window_size)\n",
        "    target_fractions = uniform_filter(target_binary, size=window_size)\n",
        "    mse_fractions = np.mean((pred_fractions - target_fractions) ** 2)\n",
        "    mse_fractions_ref = np.mean(pred_fractions ** 2) + np.mean(target_fractions ** 2)\n",
        "    return 1 - (mse_fractions / mse_fractions_ref) if mse_fractions_ref > 0 else 1.0\n",
        "\n",
        "# --- MAIN EVALUATION SCRIPT ---\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"\n",
        "    Loads the pre-trained RF model, runs it on the test set, and calculates\n",
        "    the full suite of summary statistics.\n",
        "    \"\"\"\n",
        "    # Prepare test data\n",
        "    X_test_features, Y_test_high_res_norm = load_and_prepare_data('test')\n",
        "\n",
        "    # Load the pre-trained Random Forest model\n",
        "    model_path = MODEL_DIR / 'strong_baseline_rf.joblib'\n",
        "    if not model_path.exists():\n",
        "        print(f\"FATAL: Trained RF model not found at {model_path}\")\n",
        "        print(\"Please run the training script first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nLoading pre-trained RF model from {model_path}...\")\n",
        "    rf_model = joblib.load(model_path)\n",
        "\n",
        "    # Generate predictions\n",
        "    print(\"Generating predictions with RF model...\")\n",
        "    Y_pred_flat = rf_model.predict(X_test_features)\n",
        "    Y_pred_norm = Y_pred_flat.reshape(Y_test_high_res_norm.shape)\n",
        "\n",
        "    # Calculate metrics for each sample\n",
        "    print(\"\\nCalculating metrics for each sample in the test set...\")\n",
        "    all_metrics = []\n",
        "    for i in tqdm(range(len(Y_pred_norm)), desc=\"Evaluating samples\"):\n",
        "        pred_dn = denormalize(Y_pred_norm[i], STATS)\n",
        "        true_dn = denormalize(Y_test_high_res_norm[i], STATS)\n",
        "\n",
        "        data_range = true_dn.max() - true_dn.min()\n",
        "\n",
        "        metrics = {\n",
        "            'rmse': np.sqrt(mean_squared_error(true_dn, pred_dn)),\n",
        "            'mae': mean_absolute_error(true_dn, pred_dn),\n",
        "            'r2': r2_score(true_dn, pred_dn),\n",
        "            'correlation': np.corrcoef(true_dn.flatten(), pred_dn.flatten())[0, 1],\n",
        "            'ssim': ssim(true_dn, pred_dn, data_range=data_range, win_size=7) if data_range > 0 else 1.0,\n",
        "            'csi': calculate_csi(pred_dn, true_dn),\n",
        "            'fss': calculate_fss(pred_dn, true_dn)\n",
        "        }\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "    # Compute final summary statistics in the requested format\n",
        "    print(\"\\nComputing final summary statistics...\")\n",
        "    metrics_df = pd.DataFrame(all_metrics)\n",
        "\n",
        "    final_results = {\n",
        "        'name': 'strong_rf_baseline',\n",
        "        'category': 'operational_baseline',\n",
        "        'description': 'Random Forest with Spatial Neighborhood Features',\n",
        "        'status': 'success',\n",
        "        'input_channels': 5\n",
        "    }\n",
        "\n",
        "    for metric in metrics_df.columns:\n",
        "        stats = metrics_df[metric].describe()\n",
        "        final_results[f'{metric}_mean'] = stats['mean']\n",
        "        final_results[f'{metric}_std'] = stats['std']\n",
        "        final_results[f'{metric}_median'] = stats['50%']\n",
        "        final_results[f'{metric}_min'] = stats['min']\n",
        "        final_results[f'{metric}_max'] = stats['max']\n",
        "        final_results[f'{metric}_count'] = int(stats['count'])\n",
        "\n",
        "    final_df = pd.DataFrame([final_results])\n",
        "\n",
        "    # 6. Display and save results\n",
        "    print(\"\\n--- FINAL STRONG RF BASELINE RESULTS ---\")\n",
        "    for col in final_df.columns:\n",
        "        print(f\"{col:<20}: {final_df[col].iloc[0]}\")\n",
        "\n",
        "    output_csv_path = OUTPUT_DIR / 'strong_rf_final_detailed_results.csv'\n",
        "    final_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"\\n✅ Results saved to {output_csv_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mToQK4jn8h4n",
        "outputId": "0ff2ccfa-4f44-4a9c-c206-ccc8aacbb577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Loading 'test' data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test files: 100%|██████████| 150/150 [00:01<00:00, 107.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interpolating predictors to high resolution...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Interpolating samples: 100%|██████████| 150/150 [00:05<00:00, 25.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained MOS model from /content/drive/My Drive/AR_Downscaling/publication_experiments/operational_baseline_mos/mos_model_grid.joblib...\n",
            "Generating predictions with MOS model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting pixel-wise: 100%|██████████| 65536/65536 [00:07<00:00, 8991.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating metrics for each sample in the test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 100%|██████████| 150/150 [00:01<00:00, 113.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing final summary statistics...\n",
            "\n",
            "--- FINAL MOS BASELINE RESULTS ---\n",
            "name                : mos_baseline\n",
            "category            : operational_baseline\n",
            "description         : Model Output Statistics (Pixel-wise Linear Regression)\n",
            "status              : success\n",
            "input_channels      : 5\n",
            "rmse_mean           : 94.04356138872107\n",
            "rmse_std            : 33.55237571799318\n",
            "rmse_median         : 88.28466658731905\n",
            "rmse_min            : 26.42124666609479\n",
            "rmse_max            : 194.78690279379668\n",
            "rmse_count          : 150\n",
            "mae_mean            : 79.12677005767823\n",
            "mae_std             : 31.31413909865684\n",
            "mae_median          : 72.21223831176758\n",
            "mae_min             : 20.08230209350586\n",
            "mae_max             : 182.78323364257812\n",
            "mae_count           : 150\n",
            "r2_mean             : -16.378360295395055\n",
            "r2_std              : 56.833652539966046\n",
            "r2_median           : -1.671580970287323\n",
            "r2_min              : -549.1316528320312\n",
            "r2_max              : 0.1081676185131073\n",
            "r2_count            : 150\n",
            "correlation_mean    : 0.9785047518555804\n",
            "correlation_std     : 0.016533605890518478\n",
            "correlation_median  : 0.9819267525757563\n",
            "correlation_min     : 0.9088218553743864\n",
            "correlation_max     : 0.9991399079126695\n",
            "correlation_count   : 150\n",
            "ssim_mean           : 0.9678785113269051\n",
            "ssim_std            : 0.02267676587313764\n",
            "ssim_median         : 0.9678140951461792\n",
            "ssim_min            : 0.9087924521737099\n",
            "ssim_max            : 0.9990533530454636\n",
            "ssim_count          : 150\n",
            "csi_mean            : 0.0\n",
            "csi_std             : 0.0\n",
            "csi_median          : 0.0\n",
            "csi_min             : 0.0\n",
            "csi_max             : 0.0\n",
            "csi_count           : 150\n",
            "fss_mean            : 1.0\n",
            "fss_std             : 0.0\n",
            "fss_median          : 1.0\n",
            "fss_min             : 1.0\n",
            "fss_max             : 1.0\n",
            "fss_count           : 150\n",
            "\n",
            "✅ Results saved to /content/drive/My Drive/AR_Downscaling/publication_experiments/operational_baseline_mos/mos_final_detailed_results.csv\n"
          ]
        }
      ],
      "source": [
        "# --- STANDALONE EVALUATION SCRIPT FOR MOS BASELINE ---\n",
        "# This script loads a pre-trained MOS model and evaluates it on the test set,\n",
        "# calculating a comprehensive suite of metrics and summary statistics.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from scipy.ndimage import zoom, uniform_filter\n",
        "import warnings\n",
        "\n",
        "# --- Imports for Evaluation ---\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "except:\n",
        "    PROJECT_PATH = Path('.') # For local execution\n",
        "\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "MODEL_DIR = PROJECT_PATH / 'publication_experiments' / 'operational_baseline_mos'\n",
        "OUTPUT_DIR = MODEL_DIR\n",
        "\n",
        "# Define the target high resolution\n",
        "TARGET_SHAPE = (256, 256)\n",
        "# Load normalization stats to denormalize for physical thresholds\n",
        "STATS = joblib.load(DATA_DIR / 'normalization_stats_multi_variable.joblib')\n",
        "\n",
        "# --- DATA LOADING AND PREPARATION FUNCTIONS ---\n",
        "\n",
        "def load_data(split='test'):\n",
        "    \"\"\"Loads and prepares data, cropping to a uniform size.\"\"\"\n",
        "    print(f\"Loading '{split}' data...\")\n",
        "    split_dir = DATA_DIR / split\n",
        "    predictor_files = sorted(list(split_dir.glob('*_predictor.npy')))\n",
        "    num_samples = len(predictor_files)\n",
        "    coarse_shape = np.load(predictor_files[0]).shape[1:]\n",
        "\n",
        "    X_coarse = np.zeros((num_samples, 5, *coarse_shape), dtype=np.float32)\n",
        "    Y_high_res = np.zeros((num_samples, *TARGET_SHAPE), dtype=np.float32)\n",
        "\n",
        "    for i, pred_path in enumerate(tqdm(predictor_files, desc=f\"Loading {split} files\")):\n",
        "        targ_path = Path(str(pred_path).replace('_predictor.npy', '_target.npy'))\n",
        "        predictor_data = np.load(pred_path)\n",
        "        target_data = np.load(targ_path)\n",
        "\n",
        "        h, w = target_data.shape\n",
        "        th, tw = TARGET_SHAPE\n",
        "        if h != th or w != tw:\n",
        "            start_h = max(0, (h - th) // 2)\n",
        "            start_w = max(0, (w - tw) // 2)\n",
        "            target_data = target_data[start_h : start_h + th, start_w : start_w + tw]\n",
        "\n",
        "        if target_data.shape != TARGET_SHAPE:\n",
        "             padded_target = np.zeros(TARGET_SHAPE, dtype=np.float32)\n",
        "             padded_target[:target_data.shape[0], :target_data.shape[1]] = target_data\n",
        "             target_data = padded_target\n",
        "\n",
        "        X_coarse[i] = predictor_data\n",
        "        Y_high_res[i] = target_data\n",
        "\n",
        "    return X_coarse, Y_high_res\n",
        "\n",
        "def interpolate_predictors(X_coarse, target_shape):\n",
        "    \"\"\"Upscales coarse predictor variables to the high-resolution grid.\"\"\"\n",
        "    print(\"Interpolating predictors to high resolution...\")\n",
        "    num_samples, num_channels, _, _ = X_coarse.shape\n",
        "    X_high_res = np.zeros((num_samples, num_channels, *target_shape), dtype=np.float32)\n",
        "    zoom_factors = (1, target_shape[0] / X_coarse.shape[2], target_shape[1] / X_coarse.shape[3])\n",
        "\n",
        "    for i in tqdm(range(num_samples), desc=\"Interpolating samples\"):\n",
        "        for c in range(num_channels):\n",
        "            X_high_res[i, c] = zoom(X_coarse[i, c], zoom_factors[1:], order=3)\n",
        "\n",
        "    return X_high_res\n",
        "\n",
        "def predict_with_mos(mos_model_grid, X_high_res_test):\n",
        "    \"\"\"Generates predictions using the trained MOS model grid.\"\"\"\n",
        "    print(\"Generating predictions with MOS model...\")\n",
        "    num_samples, num_channels, h, w = X_high_res_test.shape\n",
        "    X_test_reshaped = X_high_res_test.transpose(2, 3, 0, 1).reshape(h * w, num_samples, num_channels)\n",
        "    Y_pred_reshaped = np.zeros((h * w, num_samples), dtype=np.float32)\n",
        "\n",
        "    for i in tqdm(range(h * w), desc=\"Predicting pixel-wise\"):\n",
        "        row, col = i // w, i % w\n",
        "        model = mos_model_grid[row, col]\n",
        "        Y_pred_reshaped[i] = model.predict(X_test_reshaped[i])\n",
        "\n",
        "    Y_pred = Y_pred_reshaped.reshape(h, w, num_samples).transpose(2, 0, 1)\n",
        "    return Y_pred\n",
        "\n",
        "def denormalize(data, stats):\n",
        "    \"\"\"Denormalizes data back to its physical scale.\"\"\"\n",
        "    return data * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "\n",
        "def calculate_csi(pred, target, threshold=220.0):\n",
        "    \"\"\"Calculates the Critical Success Index for an event threshold.\"\"\"\n",
        "    pred_event = pred <= threshold\n",
        "    target_event = target <= threshold\n",
        "    hits = np.sum(pred_event & target_event)\n",
        "    misses = np.sum(~pred_event & target_event)\n",
        "    false_alarms = np.sum(pred_event & ~target_event)\n",
        "    return hits / (hits + misses + false_alarms) if (hits + misses + false_alarms) > 0 else 0.0\n",
        "\n",
        "def calculate_fss(pred, target, threshold=220.0, window_size=11):\n",
        "    \"\"\"Calculates the Fractions Skill Score.\"\"\"\n",
        "    pred_binary = (pred <= threshold).astype(float)\n",
        "    target_binary = (target <= threshold).astype(float)\n",
        "    pred_fractions = uniform_filter(pred_binary, size=window_size)\n",
        "    target_fractions = uniform_filter(target_binary, size=window_size)\n",
        "    mse_fractions = np.mean((pred_fractions - target_fractions) ** 2)\n",
        "    mse_fractions_ref = np.mean(pred_fractions ** 2) + np.mean(target_fractions ** 2)\n",
        "    return 1 - (mse_fractions / mse_fractions_ref) if mse_fractions_ref > 0 else 1.0\n",
        "\n",
        "\n",
        "# --- MAIN EVALUATION FUNCTION ---\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"\n",
        "    Loads the pre-trained MOS model, runs it on the test set, and calculates\n",
        "    the full suite of summary statistics.\n",
        "    \"\"\"\n",
        "    # Load test data\n",
        "    X_test_coarse, Y_test_high_res_norm = load_data('test')\n",
        "\n",
        "    # Interpolate predictors\n",
        "    X_test_interp = interpolate_predictors(X_test_coarse, TARGET_SHAPE)\n",
        "\n",
        "    # Load the pre-trained MOS model\n",
        "    model_path = MODEL_DIR / 'mos_model_grid.joblib'\n",
        "    if not model_path.exists():\n",
        "        print(f\"FATAL: Trained model not found at {model_path}\")\n",
        "        print(\"Please run the training script first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading pre-trained MOS model from {model_path}...\")\n",
        "    mos_model_grid = joblib.load(model_path)\n",
        "\n",
        "    # Generate predictions\n",
        "    Y_pred_norm = predict_with_mos(mos_model_grid, X_test_interp)\n",
        "\n",
        "    # Calculate metrics for each sample\n",
        "    print(\"Calculating metrics for each sample in the test set...\")\n",
        "    all_metrics = []\n",
        "    for i in tqdm(range(len(Y_pred_norm)), desc=\"Evaluating samples\"):\n",
        "        pred_dn = denormalize(Y_pred_norm[i], STATS)\n",
        "        true_dn = denormalize(Y_test_high_res_norm[i], STATS)\n",
        "\n",
        "        data_range = true_dn.max() - true_dn.min()\n",
        "\n",
        "        metrics = {\n",
        "            'rmse': np.sqrt(mean_squared_error(true_dn, pred_dn)),\n",
        "            'mae': mean_absolute_error(true_dn, pred_dn),\n",
        "            'r2': r2_score(true_dn, pred_dn),\n",
        "            'correlation': np.corrcoef(true_dn.flatten(), pred_dn.flatten())[0, 1],\n",
        "            'ssim': ssim(true_dn, pred_dn, data_range=data_range, win_size=7) if data_range > 0 else 1.0,\n",
        "            'csi': calculate_csi(pred_dn, true_dn),\n",
        "            'fss': calculate_fss(pred_dn, true_dn)\n",
        "        }\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "    # Compute final summary statistics and format the output\n",
        "    print(\"Computing final summary statistics...\")\n",
        "    metrics_df = pd.DataFrame(all_metrics)\n",
        "\n",
        "    final_results = {\n",
        "        'name': 'mos_baseline',\n",
        "        'category': 'operational_baseline',\n",
        "        'description': 'Model Output Statistics (Pixel-wise Linear Regression)',\n",
        "        'status': 'success',\n",
        "        'input_channels': 5\n",
        "    }\n",
        "\n",
        "    for metric in metrics_df.columns:\n",
        "        stats = metrics_df[metric].describe()\n",
        "        final_results[f'{metric}_mean'] = stats['mean']\n",
        "        final_results[f'{metric}_std'] = stats['std']\n",
        "        final_results[f'{metric}_median'] = stats['50%']\n",
        "        final_results[f'{metric}_min'] = stats['min']\n",
        "        final_results[f'{metric}_max'] = stats['max']\n",
        "        final_results[f'{metric}_count'] = int(stats['count'])\n",
        "\n",
        "    final_df = pd.DataFrame([final_results])\n",
        "\n",
        "    # Display and save results\n",
        "    print(\"\\n--- FINAL MOS BASELINE RESULTS ---\")\n",
        "    # Print in a more readable format\n",
        "    for col in final_df.columns:\n",
        "        print(f\"{col:<20}: {final_df[col].iloc[0]}\")\n",
        "\n",
        "    output_csv_path = OUTPUT_DIR / 'mos_final_detailed_results.csv'\n",
        "    final_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"\\n✅ Results saved to {output_csv_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
