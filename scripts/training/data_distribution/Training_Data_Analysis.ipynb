{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCgmv5OyQOK4",
        "outputId": "7a4e728b-4b24-487a-d6e3-023755cc8cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "🔬 Analyzing split: TRAIN\n",
            "Found 1200 train samples to analyze.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train samples: 100%|██████████| 1200/1200 [02:41<00:00,  7.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 FINAL EVENT RARITY REPORT for TRAIN\n",
            "  - T <= 230K: 46.2223% (±34.4570%)\n",
            "  - T <= 220K: 18.7507% (±19.0172%)\n",
            "  - T <= 210K: 6.6292% (±9.1181%)\n",
            "💾 Detailed per-sample results saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_statistics_train.csv\n",
            "\n",
            "🔬 Analyzing split: VAL\n",
            "Found 150 val samples to analyze.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing val samples: 100%|██████████| 150/150 [00:09<00:00, 15.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 FINAL EVENT RARITY REPORT for VAL\n",
            "  - T <= 230K: 34.5756% (±34.5958%)\n",
            "  - T <= 220K: 13.9523% (±19.7722%)\n",
            "  - T <= 210K: 4.2478% (±8.0255%)\n",
            "💾 Detailed per-sample results saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_statistics_val.csv\n",
            "\n",
            "🔬 Analyzing split: TEST\n",
            "Found 150 test samples to analyze.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test samples: 100%|██████████| 150/150 [00:08<00:00, 16.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 FINAL EVENT RARITY REPORT for TEST\n",
            "  - T <= 230K: 49.2168% (±32.6427%)\n",
            "  - T <= 220K: 19.6215% (±19.2401%)\n",
            "  - T <= 210K: 6.2629% (±9.1381%)\n",
            "💾 Detailed per-sample results saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_statistics_test.csv\n",
            "\n",
            "✅ Combined summary for all splits saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_summary_all.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "OUTPUT_DIR = PROJECT_PATH / 'training_data_stats'\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Thresholds to Analyze\n",
        "THRESHOLDS_K = [230.0, 220.0, 210.0]\n",
        "\n",
        "# Dataset splits to analyze\n",
        "SPLITS = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "def analyze_event_rarity():\n",
        "    all_split_summaries = []\n",
        "\n",
        "    for split in SPLITS:\n",
        "        print(f\"\\n🔬 Analyzing split: {split.upper()}\")\n",
        "\n",
        "        split_dir = DATA_DIR / split\n",
        "        target_files = sorted(list(split_dir.glob('*_target.npy')))\n",
        "        if not target_files:\n",
        "            print(f\"❌ No target files found in {split_dir}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Found {len(target_files)} {split} samples to analyze.\")\n",
        "\n",
        "        # Store the frequency for each sample\n",
        "        all_frequencies = []\n",
        "        for target_file in tqdm(target_files, desc=f\"Processing {split} samples\"):\n",
        "            target_k = np.load(target_file).astype(np.float32)\n",
        "            total_pixels = target_k.size\n",
        "            sample_freq = {'filename': target_file.name}\n",
        "\n",
        "            for thr in THRESHOLDS_K:\n",
        "                pixels_below_threshold = (target_k <= thr).sum()\n",
        "                frequency = (pixels_below_threshold / total_pixels) * 100\n",
        "                sample_freq[f'freq_{int(thr)}K'] = frequency\n",
        "\n",
        "            all_frequencies.append(sample_freq)\n",
        "\n",
        "        if not all_frequencies:\n",
        "            continue\n",
        "\n",
        "        # Convert results to DataFrame\n",
        "        df = pd.DataFrame(all_frequencies)\n",
        "\n",
        "        print(\"\\n📊 FINAL EVENT RARITY REPORT for\", split.upper())\n",
        "        for thr in THRESHOLDS_K:\n",
        "            col = f'freq_{int(thr)}K'\n",
        "            mean_freq = df[col].mean()\n",
        "            std_freq = df[col].std()\n",
        "            print(f\"  - T <= {int(thr)}K: {mean_freq:.4f}% (±{std_freq:.4f}%)\")\n",
        "\n",
        "        # Save per-sample results\n",
        "        per_sample_path = OUTPUT_DIR / f'event_rarity_statistics_{split}.csv'\n",
        "        df.to_csv(per_sample_path, index=False)\n",
        "        print(f\"💾 Detailed per-sample results saved to: {per_sample_path}\")\n",
        "\n",
        "        # Save summary stats for this split\n",
        "        for thr in THRESHOLDS_K:\n",
        "            col = f'freq_{int(thr)}K'\n",
        "            all_split_summaries.append({\n",
        "                'split': split,\n",
        "                'threshold': thr,\n",
        "                'mean_%': df[col].mean(),\n",
        "                'std_%': df[col].std()\n",
        "            })\n",
        "\n",
        "    # Combine and save all splits summary\n",
        "    if all_split_summaries:\n",
        "        summary_df = pd.DataFrame(all_split_summaries)\n",
        "        summary_path = OUTPUT_DIR / 'event_rarity_summary_all.csv'\n",
        "        summary_df.to_csv(summary_path, index=False)\n",
        "        print(f\"\\n✅ Combined summary for all splits saved to: {summary_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab. Assuming local file paths.\")\n",
        "\n",
        "    analyze_event_rarity()\n"
      ]
    }
  ]
}