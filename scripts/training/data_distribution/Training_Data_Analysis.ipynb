{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCgmv5OyQOK4",
        "outputId": "7a4e728b-4b24-487a-d6e3-023755cc8cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "🔬 Analyzing split: TRAIN\n",
            "Found 1200 train samples to analyze.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train samples: 100%|██████████| 1200/1200 [02:41<00:00,  7.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 FINAL EVENT RARITY REPORT for TRAIN\n",
            "  - T <= 230K: 46.2223% (±34.4570%)\n",
            "  - T <= 220K: 18.7507% (±19.0172%)\n",
            "  - T <= 210K: 6.6292% (±9.1181%)\n",
            "💾 Detailed per-sample results saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_statistics_train.csv\n",
            "\n",
            "🔬 Analyzing split: VAL\n",
            "Found 150 val samples to analyze.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing val samples: 100%|██████████| 150/150 [00:09<00:00, 15.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 FINAL EVENT RARITY REPORT for VAL\n",
            "  - T <= 230K: 34.5756% (±34.5958%)\n",
            "  - T <= 220K: 13.9523% (±19.7722%)\n",
            "  - T <= 210K: 4.2478% (±8.0255%)\n",
            "💾 Detailed per-sample results saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_statistics_val.csv\n",
            "\n",
            "🔬 Analyzing split: TEST\n",
            "Found 150 test samples to analyze.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing test samples: 100%|██████████| 150/150 [00:08<00:00, 16.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 FINAL EVENT RARITY REPORT for TEST\n",
            "  - T <= 230K: 49.2168% (±32.6427%)\n",
            "  - T <= 220K: 19.6215% (±19.2401%)\n",
            "  - T <= 210K: 6.2629% (±9.1381%)\n",
            "💾 Detailed per-sample results saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_statistics_test.csv\n",
            "\n",
            "✅ Combined summary for all splits saved to: /content/drive/My Drive/AR_Downscaling/training_data_stats/event_rarity_summary_all.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "OUTPUT_DIR = PROJECT_PATH / 'training_data_stats'\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Thresholds to Analyze\n",
        "THRESHOLDS_K = [230.0, 220.0, 210.0]\n",
        "\n",
        "# Dataset splits to analyze\n",
        "SPLITS = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "def analyze_event_rarity():\n",
        "    all_split_summaries = []\n",
        "\n",
        "    for split in SPLITS:\n",
        "        print(f\"\\n🔬 Analyzing split: {split.upper()}\")\n",
        "\n",
        "        split_dir = DATA_DIR / split\n",
        "        target_files = sorted(list(split_dir.glob('*_target.npy')))\n",
        "        if not target_files:\n",
        "            print(f\"❌ No target files found in {split_dir}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Found {len(target_files)} {split} samples to analyze.\")\n",
        "\n",
        "        # Store the frequency for each sample\n",
        "        all_frequencies = []\n",
        "        for target_file in tqdm(target_files, desc=f\"Processing {split} samples\"):\n",
        "            target_k = np.load(target_file).astype(np.float32)\n",
        "            total_pixels = target_k.size\n",
        "            sample_freq = {'filename': target_file.name}\n",
        "\n",
        "            for thr in THRESHOLDS_K:\n",
        "                pixels_below_threshold = (target_k <= thr).sum()\n",
        "                frequency = (pixels_below_threshold / total_pixels) * 100\n",
        "                sample_freq[f'freq_{int(thr)}K'] = frequency\n",
        "\n",
        "            all_frequencies.append(sample_freq)\n",
        "\n",
        "        if not all_frequencies:\n",
        "            continue\n",
        "\n",
        "        # Convert results to DataFrame\n",
        "        df = pd.DataFrame(all_frequencies)\n",
        "\n",
        "        print(\"\\n📊 FINAL EVENT RARITY REPORT for\", split.upper())\n",
        "        for thr in THRESHOLDS_K:\n",
        "            col = f'freq_{int(thr)}K'\n",
        "            mean_freq = df[col].mean()\n",
        "            std_freq = df[col].std()\n",
        "            print(f\"  - T <= {int(thr)}K: {mean_freq:.4f}% (±{std_freq:.4f}%)\")\n",
        "\n",
        "        # Save per-sample results\n",
        "        per_sample_path = OUTPUT_DIR / f'event_rarity_statistics_{split}.csv'\n",
        "        df.to_csv(per_sample_path, index=False)\n",
        "        print(f\"💾 Detailed per-sample results saved to: {per_sample_path}\")\n",
        "\n",
        "        # Save summary stats for this split\n",
        "        for thr in THRESHOLDS_K:\n",
        "            col = f'freq_{int(thr)}K'\n",
        "            all_split_summaries.append({\n",
        "                'split': split,\n",
        "                'threshold': thr,\n",
        "                'mean_%': df[col].mean(),\n",
        "                'std_%': df[col].std()\n",
        "            })\n",
        "\n",
        "    # Combine and save all splits summary\n",
        "    if all_split_summaries:\n",
        "        summary_df = pd.DataFrame(all_split_summaries)\n",
        "        summary_path = OUTPUT_DIR / 'event_rarity_summary_all.csv'\n",
        "        summary_df.to_csv(summary_path, index=False)\n",
        "        print(f\"\\n✅ Combined summary for all splits saved to: {summary_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab. Assuming local file paths.\")\n",
        "\n",
        "    analyze_event_rarity()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "OUTPUT_DIR = PROJECT_PATH / 'hip_eef_analysis_results'\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Analysis Parameters ---\n",
        "THRESHOLDS_TO_ANALYZE = [220.0, 210.0] # Kelvin\n",
        "\n",
        "# --- 2. MAIN ANALYSIS SCRIPT ---\n",
        "def analyze_data_distribution():\n",
        "    \"\"\"\n",
        "    Iterates through the entire training dataset to analyze the distribution\n",
        "    of rare event pixels.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Training Data Distribution Analysis ---\")\n",
        "\n",
        "    # --- Load the training dataset ---\n",
        "    # We don't need predictors, so we can modify the dataset logic slightly\n",
        "    # for speed if necessary, but for now we'll use the existing class.\n",
        "    train_dataset = MultiVariableARDataset(DATA_DIR, 'train')\n",
        "    stats = train_dataset.stats\n",
        "    mean, std = stats['target_mean'], stats['target_std'] + 1e-8\n",
        "\n",
        "    pixel_percentages = {thresh: [] for thresh in THRESHOLDS_TO_ANALYZE}\n",
        "    total_pixels_per_sample = 0\n",
        "\n",
        "    print(f\"Analyzing {len(train_dataset)} training samples...\")\n",
        "    pbar = tqdm(train_dataset, desc=\"Processing samples\")\n",
        "    for _, target_norm, _ in pbar:\n",
        "\n",
        "        ground_truth_k = target_norm.numpy().squeeze() * std + mean\n",
        "\n",
        "        if total_pixels_per_sample == 0:\n",
        "            total_pixels_per_sample = ground_truth_k.size\n",
        "\n",
        "        for thresh in THRESHOLDS_TO_ANALYZE:\n",
        "            cold_pixels = (ground_truth_k <= thresh).sum()\n",
        "            percentage = (cold_pixels / total_pixels_per_sample) * 100\n",
        "            pixel_percentages[thresh].append(percentage)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"                 Training Data Distribution Summary\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for thresh in THRESHOLDS_TO_ANALYZE:\n",
        "        percentages = np.array(pixel_percentages[thresh])\n",
        "        print(f\"\\n--- Analysis for Threshold <= {thresh}K ---\")\n",
        "        print(f\"  - Mean Percentage Across All Samples: {percentages.mean():.2f}%\")\n",
        "        print(f\"  - Median Percentage Across All Samples: {np.median(percentages):.2f}%\")\n",
        "        print(f\"  - Max Percentage in a Single Sample: {percentages.max():.2f}%\")\n",
        "        print(f\"  - Samples with ZERO relevant pixels: {np.sum(percentages == 0)} / {len(percentages)} ({ (np.sum(percentages == 0)/len(percentages))*100:.1f}%)\")\n",
        "        print(f\"  - Samples with < 1% relevant pixels: {np.sum(percentages < 1)} / {len(percentages)} ({ (np.sum(percentages < 1)/len(percentages))*100:.1f}%)\")\n",
        "\n",
        "    # --- Visualization ---\n",
        "    fig, axes = plt.subplots(1, len(THRESHOLDS_TO_ANALYZE), figsize=(16, 6), sharey=True)\n",
        "    fig.suptitle('Distribution of Extreme Event Pixels in Training Data', fontsize=20)\n",
        "\n",
        "    for i, thresh in enumerate(THRESHOLDS_TO_ANALYZE):\n",
        "        ax = axes[i]\n",
        "        percentages = pixel_percentages[thresh]\n",
        "        ax.hist(percentages, bins=50, range=(0, max(10, np.max(percentages))), edgecolor='black')\n",
        "        ax.set_title(f'Threshold <= {thresh}K', fontsize=16)\n",
        "        ax.set_xlabel('Percentage of Pixels in Sample (%)', fontsize=12)\n",
        "        if i == 0:\n",
        "            ax.set_ylabel('Number of Training Samples', fontsize=12)\n",
        "        ax.grid(True, linestyle='--', alpha=0.6)\n",
        "        ax.axvline(np.mean(percentages), color='r', linestyle='--', linewidth=2, label=f'Mean ({np.mean(percentages):.2f}%)')\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    save_path = OUTPUT_DIR / 'training_data_distribution.png'\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    print(f\"\\n✅ Distribution plot saved to: {save_path}\")\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if MultiVariableARDataset is not None:\n",
        "        analyze_data_distribution()\n",
        "    else:\n",
        "        print(\"Analysis script aborted due to missing dataset class.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
