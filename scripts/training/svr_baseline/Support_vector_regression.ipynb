{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkuGhv0C8Pdn",
        "outputId": "b2fd9812-5340-48cf-f647-57a9e3d6d19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üöÄ SUPPORT VECTOR REGRESSION BASELINE EXPERIMENT\n",
            "======================================================================\n",
            "üéØ Training Support Vector Regression Baseline (Pixel-wise)\n",
            "============================================================\n",
            "üìÇ Loading 1200 samples for pixel-wise SVR training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading training samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [04:43<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined data shape: X=(21145200, 5), y=(21145200,)\n",
            "Training SVR on 100000 pixel samples from ALL 1200 training images...\n",
            "üîç Training SVR with grid search...\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "‚úÖ Best parameters: {'C': 1.0, 'epsilon': 0.2, 'gamma': 0.01}\n",
            "‚úÖ Best CV score: 158.1168\n",
            "üíæ Model saved to /content/drive/My Drive/AR_Downscaling/publication_experiments/svr_baseline/svr_baseline.joblib\n",
            "‚úÖ SVR training completed successfully!\n",
            "\n",
            "üìä Evaluating SVR on test set...\n",
            "Evaluating on 150 test samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [18:47<00:00,  7.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìà Computing summary statistics from 150 successful evaluations...\n",
            "\n",
            "--- FINAL SVR BASELINE RESULTS ---\n",
            "name                : svr_baseline\n",
            "category            : traditional_ml\n",
            "description         : Support Vector Regression with RBF Kernel\n",
            "status              : success\n",
            "input_channels      : 5\n",
            "rmse_mean           : 128.3721335476796\n",
            "rmse_std            : 48.10959965138197\n",
            "rmse_median         : 114.99524073242266\n",
            "rmse_min            : 32.32572389960285\n",
            "rmse_max            : 262.47591615840906\n",
            "rmse_count          : 150\n",
            "mae_mean            : 116.58302261899223\n",
            "mae_std             : 51.105274346056056\n",
            "mae_median          : 102.54400018602169\n",
            "mae_min             : 26.403133483944593\n",
            "mae_max             : 261.9155447538069\n",
            "mae_count           : 150\n",
            "r2_mean             : -19.577696883728734\n",
            "r2_std              : 61.85530263241797\n",
            "r2_median           : -0.7232468872234958\n",
            "r2_min              : -550.2601274040269\n",
            "r2_max              : 0.02154379543104068\n",
            "r2_count            : 150\n",
            "correlation_mean    : 0.04817253644870113\n",
            "correlation_std     : 0.2044871178698866\n",
            "correlation_median  : 0.03668811081582735\n",
            "correlation_min     : -0.6007989572256912\n",
            "correlation_max     : 0.5956943450865758\n",
            "correlation_count   : 150\n",
            "ssim_mean           : 0.6417253176030044\n",
            "ssim_std            : 0.11042582959857364\n",
            "ssim_median         : 0.6419672477164412\n",
            "ssim_min            : 0.29318107038739694\n",
            "ssim_max            : 0.9521785559769884\n",
            "ssim_count          : 150\n",
            "csi_mean            : 0.0\n",
            "csi_std             : 0.0\n",
            "csi_median          : 0.0\n",
            "csi_min             : 0.0\n",
            "csi_max             : 0.0\n",
            "csi_count           : 150\n",
            "fss_mean            : 1.0\n",
            "fss_std             : 0.0\n",
            "fss_median          : 1.0\n",
            "fss_min             : 1.0\n",
            "fss_max             : 1.0\n",
            "fss_count           : 150\n",
            "csi_210_mean        : 0.0\n",
            "csi_210_std         : 0.0\n",
            "csi_210_median      : 0.0\n",
            "csi_210_min         : 0.0\n",
            "csi_210_max         : 0.0\n",
            "csi_210_count       : 150\n",
            "csi_220_mean        : 0.0\n",
            "csi_220_std         : 0.0\n",
            "csi_220_median      : 0.0\n",
            "csi_220_min         : 0.0\n",
            "csi_220_max         : 0.0\n",
            "csi_220_count       : 150\n",
            "csi_230_mean        : 0.0\n",
            "csi_230_std         : 0.0\n",
            "csi_230_median      : 0.0\n",
            "csi_230_min         : 0.0\n",
            "csi_230_max         : 0.0\n",
            "csi_230_count       : 150\n",
            "\n",
            "‚úÖ Results saved to /content/drive/My Drive/AR_Downscaling/publication_experiments/svr_baseline/svr_detailed_results.csv\n",
            "‚úÖ SVR evaluation completed successfully!\n",
            "\n",
            "======================================================================\n",
            "üéâ SVR BASELINE EXPERIMENT COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "üìä KEY RESULTS:\n",
            "  SSIM: 0.642 ¬± 0.110\n",
            "  Correlation: 0.048 ¬± 0.204\n",
            "  CSI: 0.000 ¬± 0.000\n",
            "  FSS: 1.000 ¬± 0.000\n",
            "\n",
            "üîç VALIDATION AGAINST PAPER CLAIMS:\n",
            "  SSIM: Expected 0.887, Got 0.642\n",
            "  Correlation: Expected 0.921, Got 0.048\n",
            "  CSI: Expected 0.012, Got 0.000\n",
            "\n",
            "üìÅ All outputs saved to: /content/drive/My Drive/AR_Downscaling/publication_experiments/svr_baseline\n",
            "\n",
            "üéä SVR BASELINE READY FOR COMPREHENSIVE EVALUATION!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Fixed Support Vector Regression Baseline Implementation\n",
        "Uses pixel-wise prediction approach for spatial field reconstruction\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from scipy.ndimage import uniform_filter\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "except:\n",
        "    PROJECT_PATH = Path('.')\n",
        "\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "OUTPUT_DIR = PROJECT_PATH / 'publication_experiments' / 'svr_baseline'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_sample_data(n_samples=None):\n",
        "    \"\"\"Load data for SVR training (pixel-wise approach).\"\"\"\n",
        "\n",
        "    # Load training data\n",
        "    train_dir = DATA_DIR / 'train'\n",
        "    predictor_files = sorted(list(train_dir.glob('*_predictor.npy')))\n",
        "\n",
        "    if n_samples is not None:\n",
        "        predictor_files = predictor_files[:n_samples]\n",
        "\n",
        "    print(f\"üìÇ Loading {len(predictor_files)} samples for pixel-wise SVR training...\")\n",
        "\n",
        "    X_data = []\n",
        "    y_data = []\n",
        "\n",
        "    for pred_file in tqdm(predictor_files, desc=\"Loading training samples\"):\n",
        "        # Load predictor and target\n",
        "        pred_data = np.load(pred_file)  # Shape: (5, H, W)\n",
        "        target_file = Path(str(pred_file).replace('_predictor.npy', '_target.npy'))\n",
        "        target_data = np.load(target_file)  # Shape: (H, W)\n",
        "\n",
        "        # Reshape for pixel-wise training\n",
        "        H, W = target_data.shape\n",
        "\n",
        "        # For each pixel, create features from predictor variables\n",
        "        pred_reshaped = pred_data.reshape(5, -1).T  # Shape: (H*W, 5)\n",
        "        target_reshaped = target_data.reshape(-1)   # Shape: (H*W,)\n",
        "\n",
        "        # Subsample pixels for efficiency (every 5th pixel for better coverage)\n",
        "        pixel_indices = np.arange(0, len(target_reshaped), 5)  # Every 5th pixel instead of 10th\n",
        "\n",
        "        X_data.append(pred_reshaped[pixel_indices])\n",
        "        y_data.append(target_reshaped[pixel_indices])\n",
        "\n",
        "    X_combined = np.vstack(X_data)\n",
        "    y_combined = np.hstack(y_data)\n",
        "\n",
        "    print(f\"Combined data shape: X={X_combined.shape}, y={y_combined.shape}\")\n",
        "    return X_combined, y_combined\n",
        "\n",
        "def train_svr_model():\n",
        "    \"\"\"Train SVR model with pixel-wise approach.\"\"\"\n",
        "\n",
        "    print(\"üéØ Training Support Vector Regression Baseline (Pixel-wise)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load training data - USE ALL 1200 SAMPLES\n",
        "    X_train, y_train = load_sample_data(n_samples=None)  # Use ALL samples\n",
        "\n",
        "    # Subsample pixels for SVR efficiency, but keep more data\n",
        "    n_train = min(100000, len(X_train))  # Increased from 20k to 100k pixels\n",
        "    indices = np.random.choice(len(X_train), n_train, replace=False)\n",
        "    X_train_sub = X_train[indices]\n",
        "    y_train_sub = y_train[indices]\n",
        "\n",
        "    print(f\"Training SVR on {len(X_train_sub)} pixel samples from ALL 1200 training images...\")\n",
        "\n",
        "    # Simplified hyperparameter grid for faster training\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1.0, 10.0],\n",
        "        'epsilon': [0.1, 0.2],\n",
        "        'gamma': ['scale', 0.01]\n",
        "    }\n",
        "\n",
        "    # Train SVR\n",
        "    print(\"üîç Training SVR with grid search...\")\n",
        "    svr = SVR(kernel='rbf', max_iter=1000)\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        svr,\n",
        "        param_grid,\n",
        "        cv=3,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "    print(f\"‚úÖ Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"‚úÖ Best CV score: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model_path = OUTPUT_DIR / 'svr_baseline.joblib'\n",
        "    joblib.dump(grid_search.best_estimator_, model_path)\n",
        "\n",
        "    # Load stats\n",
        "    stats_file = DATA_DIR / 'normalization_stats_multi_variable.joblib'\n",
        "    stats = joblib.load(stats_file)\n",
        "\n",
        "    print(f\"üíæ Model saved to {model_path}\")\n",
        "\n",
        "    return grid_search.best_estimator_, stats\n",
        "\n",
        "def predict_spatial_field(svr_model, predictor_data):\n",
        "    \"\"\"Predict full spatial field using trained SVR model.\"\"\"\n",
        "\n",
        "    # predictor_data shape: (5, H, W)\n",
        "    n_vars, H, W = predictor_data.shape\n",
        "\n",
        "    # Reshape for pixel-wise prediction\n",
        "    pred_reshaped = predictor_data.reshape(n_vars, -1).T  # Shape: (H*W, 5)\n",
        "\n",
        "    # Predict each pixel\n",
        "    predictions = svr_model.predict(pred_reshaped)  # Shape: (H*W,)\n",
        "\n",
        "    # Reshape back to spatial field\n",
        "    predicted_field = predictions.reshape(H, W)\n",
        "\n",
        "    return predicted_field\n",
        "\n",
        "def calculate_csi(pred, target, threshold=220.0):\n",
        "    \"\"\"Calculate Critical Success Index.\"\"\"\n",
        "    pred_event = pred <= threshold\n",
        "    target_event = target <= threshold\n",
        "    hits = np.sum(pred_event & target_event)\n",
        "    misses = np.sum(~pred_event & target_event)\n",
        "    false_alarms = np.sum(pred_event & ~target_event)\n",
        "    return hits / (hits + misses + false_alarms) if (hits + misses + false_alarms) > 0 else 0.0\n",
        "\n",
        "def calculate_fss(pred, target, threshold=220.0, window_size=11):\n",
        "    \"\"\"Calculate Fractions Skill Score.\"\"\"\n",
        "    pred_binary = (pred <= threshold).astype(float)\n",
        "    target_binary = (target <= threshold).astype(float)\n",
        "    pred_fractions = uniform_filter(pred_binary, size=window_size)\n",
        "    target_fractions = uniform_filter(target_binary, size=window_size)\n",
        "    mse_fractions = np.mean((pred_fractions - target_fractions) ** 2)\n",
        "    mse_fractions_ref = np.mean(pred_fractions ** 2) + np.mean(target_fractions ** 2)\n",
        "    return 1 - (mse_fractions / mse_fractions_ref) if mse_fractions_ref > 0 else 1.0\n",
        "\n",
        "def denormalize(data, stats):\n",
        "    \"\"\"Denormalize data using provided statistics.\"\"\"\n",
        "    return data * (stats.get('target_std', 1.0) + 1e-8) + stats.get('target_mean', 0.0)\n",
        "\n",
        "def evaluate_svr_model():\n",
        "    \"\"\"Evaluate SVR model on test set.\"\"\"\n",
        "\n",
        "    print(\"\\nüìä Evaluating SVR on test set...\")\n",
        "\n",
        "    # Load model and stats\n",
        "    model_path = OUTPUT_DIR / 'svr_baseline.joblib'\n",
        "    if not model_path.exists():\n",
        "        print(\"‚ùå SVR model not found. Please train first.\")\n",
        "        return\n",
        "\n",
        "    svr_model = joblib.load(model_path)\n",
        "    stats_file = DATA_DIR / 'normalization_stats_multi_variable.joblib'\n",
        "    stats = joblib.load(stats_file)\n",
        "\n",
        "    # Load test data\n",
        "    test_dir = DATA_DIR / 'test'\n",
        "    predictor_files = sorted(list(test_dir.glob('*_predictor.npy')))\n",
        "\n",
        "    print(f\"Evaluating on {len(predictor_files)} test samples...\")\n",
        "\n",
        "    all_metrics = []\n",
        "\n",
        "    for pred_file in tqdm(predictor_files, desc=\"Evaluating samples\"):\n",
        "        try:\n",
        "            # Load data\n",
        "            pred_data = np.load(pred_file)  # Shape: (5, H, W)\n",
        "            target_file = Path(str(pred_file).replace('_predictor.npy', '_target.npy'))\n",
        "            target_data = np.load(target_file)  # Shape: (H, W)\n",
        "\n",
        "            # Predict spatial field\n",
        "            predicted_field = predict_spatial_field(svr_model, pred_data)\n",
        "\n",
        "            # Denormalize for physical metrics\n",
        "            pred_dn = denormalize(predicted_field, stats)\n",
        "            target_dn = denormalize(target_data, stats)\n",
        "\n",
        "            # Calculate metrics\n",
        "            data_range = target_dn.max() - target_dn.min()\n",
        "\n",
        "            # Handle edge case where correlation might fail\n",
        "            try:\n",
        "                correlation = np.corrcoef(target_dn.flatten(), pred_dn.flatten())[0, 1]\n",
        "                if np.isnan(correlation):\n",
        "                    correlation = 0.0\n",
        "            except:\n",
        "                correlation = 0.0\n",
        "\n",
        "            metrics = {\n",
        "                'rmse': np.sqrt(mean_squared_error(target_dn.flatten(), pred_dn.flatten())),\n",
        "                'mae': mean_absolute_error(target_dn.flatten(), pred_dn.flatten()),\n",
        "                'r2': r2_score(target_dn.flatten(), pred_dn.flatten()),\n",
        "                'correlation': correlation,\n",
        "                'ssim': ssim(target_dn, pred_dn, data_range=data_range, win_size=7) if data_range > 0 else 1.0,\n",
        "                'csi': calculate_csi(pred_dn, target_dn),\n",
        "                'fss': calculate_fss(pred_dn, target_dn)\n",
        "            }\n",
        "\n",
        "            for temp in [210, 220, 230]:\n",
        "                metrics[f'csi_{temp}'] = calculate_csi(pred_dn, target_dn, threshold=temp)\n",
        "\n",
        "\n",
        "            all_metrics.append(metrics)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error processing {pred_file.name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not all_metrics:\n",
        "        print(\"‚ùå No successful evaluations completed\")\n",
        "        return None\n",
        "\n",
        "    # Compute summary statistics\n",
        "    print(f\"\\nüìà Computing summary statistics from {len(all_metrics)} successful evaluations...\")\n",
        "    metrics_df = pd.DataFrame(all_metrics)\n",
        "\n",
        "    final_results = {\n",
        "        'name': 'svr_baseline',\n",
        "        'category': 'traditional_ml',\n",
        "        'description': 'Support Vector Regression with RBF Kernel',\n",
        "        'status': 'success',\n",
        "        'input_channels': 5\n",
        "    }\n",
        "\n",
        "    for metric in metrics_df.columns:\n",
        "        stats_summary = metrics_df[metric].describe()\n",
        "        final_results[f'{metric}_mean'] = stats_summary['mean']\n",
        "        final_results[f'{metric}_std'] = stats_summary['std']\n",
        "        final_results[f'{metric}_median'] = stats_summary['50%']\n",
        "        final_results[f'{metric}_min'] = stats_summary['min']\n",
        "        final_results[f'{metric}_max'] = stats_summary['max']\n",
        "        final_results[f'{metric}_count'] = int(stats_summary['count'])\n",
        "\n",
        "    # Save results\n",
        "    final_df = pd.DataFrame([final_results])\n",
        "    results_path = OUTPUT_DIR / 'svr_detailed_results.csv'\n",
        "    final_df.to_csv(results_path, index=False)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n--- FINAL SVR BASELINE RESULTS ---\")\n",
        "    for col in final_df.columns:\n",
        "        print(f\"{col:<20}: {final_df[col].iloc[0]}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Results saved to {results_path}\")\n",
        "\n",
        "    return final_results\n",
        "\n",
        "def run_complete_svr_experiment():\n",
        "    \"\"\"Run complete SVR baseline experiment.\"\"\"\n",
        "\n",
        "    print(\"üöÄ SUPPORT VECTOR REGRESSION BASELINE EXPERIMENT\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Train model\n",
        "    try:\n",
        "        svr_model, stats = train_svr_model()\n",
        "        print(\"‚úÖ SVR training completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå SVR training failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    # Evaluate model\n",
        "    try:\n",
        "        results = evaluate_svr_model()\n",
        "        if results:\n",
        "            print(\"‚úÖ SVR evaluation completed successfully!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è SVR evaluation completed with issues\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå SVR evaluation failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üéâ SVR BASELINE EXPERIMENT COMPLETED!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if results:\n",
        "        print(f\"\\nüìä KEY RESULTS:\")\n",
        "        print(f\"  SSIM: {results['ssim_mean']:.3f} ¬± {results['ssim_std']:.3f}\")\n",
        "        print(f\"  Correlation: {results['correlation_mean']:.3f} ¬± {results['correlation_std']:.3f}\")\n",
        "        print(f\"  CSI: {results['csi_mean']:.3f} ¬± {results['csi_std']:.3f}\")\n",
        "        print(f\"  FSS: {results['fss_mean']:.3f} ¬± {results['fss_std']:.3f}\")\n",
        "\n",
        "        # Check if results match expected values from paper\n",
        "        expected_ssim = 0.887\n",
        "        expected_corr = 0.921\n",
        "        expected_csi = 0.012\n",
        "\n",
        "        print(f\"\\nüîç VALIDATION AGAINST PAPER CLAIMS:\")\n",
        "        print(f\"  SSIM: Expected {expected_ssim:.3f}, Got {results['ssim_mean']:.3f}\")\n",
        "        print(f\"  Correlation: Expected {expected_corr:.3f}, Got {results['correlation_mean']:.3f}\")\n",
        "        print(f\"  CSI: Expected {expected_csi:.3f}, Got {results['csi_mean']:.3f}\")\n",
        "\n",
        "    print(f\"\\nüìÅ All outputs saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete SVR experiment\n",
        "    results = run_complete_svr_experiment()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nüéä SVR BASELINE READY FOR COMPREHENSIVE EVALUATION!\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è SVR experiment completed with issues. Check logs above.\")"
      ]
    }
  ]
}