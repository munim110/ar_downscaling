{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3STzxKVtiC2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Re-using the ResBlock, Attention, and DecoderBlock from previous model ---\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1), nn.BatchNorm2d(out_channels))\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x); out = F.relu(self.bn1(self.conv1(x))); out = self.bn2(self.conv2(out)); out += identity; return F.relu(out)\n",
        "\n",
        "class SimplifiedAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__(); self.attention = nn.Sequential(nn.Conv2d(channels, channels // 8, 1), nn.ReLU(True), nn.Conv2d(channels // 8, channels, 1), nn.Sigmoid())\n",
        "    def forward(self, x): return x * self.attention(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels, use_attention=False):\n",
        "        super().__init__(); self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2); self.res_block = ResBlock(out_channels + skip_channels, out_channels); self.attention = SimplifiedAttention(out_channels) if use_attention else nn.Identity()\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        if x.shape != skip.shape: x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, skip], dim=1); x = self.res_block(x); return self.attention(x)\n",
        "\n",
        "class HiP_EEF_Direct(nn.Module):\n",
        "    \"\"\"\n",
        "    Final HiP-EEF architecture for Direct Residual Prediction.\n",
        "    This version removes the Gating Head for simpler, direct supervision.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels=4, base_c=32):\n",
        "        super().__init__()\n",
        "        # --- Shared Encoder (Unchanged) ---\n",
        "        self.enc1 = ResBlock(n_channels, base_c); self.enc2 = ResBlock(base_c, base_c * 2); self.enc3 = ResBlock(base_c * 2, base_c * 4); self.enc4 = ResBlock(base_c * 4, base_c * 8); self.down = nn.MaxPool2d(2)\n",
        "        self.bottleneck = ResBlock(base_c * 8, base_c * 16)\n",
        "\n",
        "        # --- Continuity Head Decoder (Unchanged) ---\n",
        "        self.up1_cont = DecoderBlock(base_c * 16, base_c * 8, base_c * 8); self.up2_cont = DecoderBlock(base_c * 8, base_c * 4, base_c * 4); self.up3_cont = DecoderBlock(base_c * 4, base_c * 2, base_c * 2); self.up4_cont = DecoderBlock(base_c * 2, base_c, base_c); self.out_cont = nn.Conv2d(base_c, 1, kernel_size=1)\n",
        "\n",
        "        # --- Extreme Event Head Decoder (Unchanged) ---\n",
        "        self.up1_ext = DecoderBlock(base_c * 16, base_c * 8, base_c * 8); self.up2_ext = DecoderBlock(base_c * 8, base_c * 4, base_c * 4, use_attention=True); self.up3_ext = DecoderBlock(base_c * 4, base_c * 2, base_c * 2); self.up4_ext = DecoderBlock(base_c * 2, base_c, base_c); self.out_ext = nn.Conv2d(base_c, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- Encoder Path ---\n",
        "        s1 = self.enc1(x); s2 = self.enc2(self.down(s1)); s3 = self.enc3(self.down(s2)); s4 = self.enc4(self.down(s3))\n",
        "        b = self.bottleneck(self.down(s4))\n",
        "\n",
        "        # --- Decoder Paths ---\n",
        "        c4 = self.up1_cont(b, s4); c3 = self.up2_cont(c4, s3); c2 = self.up3_cont(c3, s2); c1 = self.up4_cont(c2, s1)\n",
        "        cont_pred = self.out_cont(c1)\n",
        "\n",
        "        e4 = self.up1_ext(b, s4); e3 = self.up2_ext(e4, s3); e2 = self.up3_ext(e3, s2); e1 = self.up4_ext(e2, s1)\n",
        "        ext_pred = self.out_ext(e1)\n",
        "\n",
        "        # --- Simple Fusion ---\n",
        "        final_pred = cont_pred + ext_pred\n",
        "\n",
        "        return final_pred, cont_pred, ext_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "class MultiVariableARDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A flexible dataset for the AR Downscaling project that loads all available\n",
        "    predictor variables and the target variable.\n",
        "\n",
        "    The selection of specific variables (e.g., for ablation or the HiP-EEF model)\n",
        "    is handled within the training script, not in the dataset itself.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir: Path, split: str):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            data_dir (Path): The root directory of the dataset containing the splits.\n",
        "            split (str): The dataset split to load ('train', 'val', or 'test').\n",
        "        \"\"\"\n",
        "        self.split_dir = Path(data_dir) / split\n",
        "        if not self.split_dir.exists():\n",
        "            raise FileNotFoundError(f\"Dataset split directory not found: {self.split_dir}\")\n",
        "\n",
        "        self.predictor_files = sorted(list(self.split_dir.glob('*_predictor.npy')))\n",
        "        if not self.predictor_files:\n",
        "            raise FileNotFoundError(f\"No predictor files found in {self.split_dir}\")\n",
        "\n",
        "        stats_path = Path(data_dir) / 'normalization_stats_multi_variable.joblib'\n",
        "        if not stats_path.exists():\n",
        "            raise FileNotFoundError(f\"Normalization stats file not found: {stats_path}\")\n",
        "\n",
        "        self.stats = joblib.load(stats_path)\n",
        "        print(f\"Loaded '{split}' dataset with {len(self.predictor_files)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.predictor_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single sample (predictor, target, case_name) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - torch.Tensor: The full normalized predictor variables.\n",
        "                - torch.Tensor: The normalized target variable.\n",
        "                - str: The name of the case for identification.\n",
        "        \"\"\"\n",
        "        pred_path = self.predictor_files[idx]\n",
        "        targ_path = Path(str(pred_path).replace('_predictor.npy', '_target.npy'))\n",
        "        case_name = pred_path.stem.replace('_predictor', '')\n",
        "\n",
        "        # Load the full 5-channel predictor and single-channel target\n",
        "        full_predictor = np.load(pred_path).astype(np.float32)\n",
        "        target_data = np.load(targ_path).astype(np.float32)\n",
        "\n",
        "        # Normalize using the pre-calculated stats\n",
        "        # Add new axes for broadcasting (C, 1, 1)\n",
        "        predictor_norm = (full_predictor - self.stats['predictor_mean'][:, None, None]) / \\\n",
        "                         (self.stats['predictor_std'][:, None, None] + 1e-8)\n",
        "\n",
        "        target_norm = (target_data - self.stats['target_mean']) / \\\n",
        "                      (self.stats['target_std'] + 1e-8)\n",
        "\n",
        "        # Convert to PyTorch tensors and add channel dimension to target\n",
        "        return (\n",
        "            torch.from_numpy(predictor_norm),\n",
        "            torch.from_numpy(target_norm).unsqueeze(0),\n",
        "            case_name\n",
        "        )\n",
        "\n",
        "def calculate_validation_csi(model, val_loader, device, stats, threshold_k=220.0):\n",
        "    model.eval()\n",
        "    all_csi = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for predictor, target_norm, _ in val_loader:\n",
        "            predictor_subset = predictor[:, VARIABLE_INDICES, :, :].to(device)\n",
        "            target_norm = target_norm.to(device)\n",
        "\n",
        "            final_pred, _, _ = model(predictor_subset)\n",
        "\n",
        "            # Convert to Kelvin\n",
        "            pred_k = final_pred.cpu().numpy() * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "            target_k = target_norm.cpu().numpy() * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "\n",
        "            for i in range(pred_k.shape[0]):\n",
        "                pred_mask = pred_k[i].squeeze() <= threshold_k\n",
        "                target_mask = target_k[i].squeeze() <= threshold_k\n",
        "\n",
        "                hits = np.sum(pred_mask & target_mask)\n",
        "                misses = np.sum(~pred_mask & target_mask)\n",
        "                false_alarms = np.sum(pred_mask & ~target_mask)\n",
        "\n",
        "                if (hits + misses + false_alarms) > 0:\n",
        "                    csi = hits / (hits + misses + false_alarms)\n",
        "                    all_csi.append(csi)\n",
        "\n",
        "    return np.mean(all_csi) if all_csi else 0.0"
      ],
      "metadata": {
        "id": "UWxf05-6vB4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conservative Training with simple MSE loss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "OUTPUT_DIR = PROJECT_PATH / 'hip_eef_direct_model'\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "MODEL_SAVE_PATH = OUTPUT_DIR / 'hip_eef_direct_model.pth'\n",
        "WEIGHTS_SAVE_PATH = PROJECT_PATH / 'hip_eef_smart_sampling_model' / 'sampler_weights.pt'\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "EPOCHS = 50; BATCH_SIZE = 8; LEARNING_RATE = 1e-4; EARLY_STOPPING_PATIENCE = 10; GRADIENT_CLIP = 1.0\n",
        "\n",
        "# --- HiP-EEF Variable & Loss Configuration ---\n",
        "ALL_VARIABLES = ['IVT', 'T500', 'T850', 'RH700', 'W500']; VARIABLE_NAMES = ['T500', 'T850', 'RH700', 'W500']\n",
        "VARIABLE_INDICES = [ALL_VARIABLES.index(v) for v in VARIABLE_NAMES]; INPUT_CHANNELS = len(VARIABLE_INDICES)\n",
        "\n",
        "# --- Loss Weights for Direct Supervision ---\n",
        "ALPHA = 0.5  # Continuity Head (predicting capped background)\n",
        "BETA = 1.5   # Extreme Head (predicting storm residual)\n",
        "DELTA = 0.4  # Final Fused Prediction (overall accuracy)\n",
        "\n",
        "# --- Direct Supervision Parameters ---\n",
        "RESIDUAL_THRESHOLD_K = 225.0\n",
        "SAMPLING_WEIGHT_THRESHOLD = 220.0\n",
        "\n",
        "# --- 2. GROUND TRUTH DECOMPOSITION & SAMPLER ---\n",
        "def decompose_ground_truth(target_k, threshold_k):\n",
        "    background_gt = torch.clamp(target_k, min=threshold_k)\n",
        "    extreme_residual_gt = target_k - background_gt # This will be <= 0\n",
        "    return background_gt, extreme_residual_gt\n",
        "\n",
        "def get_sampler_weights(dataset, stats):\n",
        "    if WEIGHTS_SAVE_PATH.exists():\n",
        "        print(f\"Loading cached sampler weights from {WEIGHTS_SAVE_PATH}\")\n",
        "        return torch.load(WEIGHTS_SAVE_PATH)\n",
        "    # ... (sampler calculation is the same) ...\n",
        "    print(\"Pre-computing sampler weights...\"); weights = []\n",
        "    for _, target_norm, _ in tqdm(dataset):\n",
        "        target_k = target_norm.numpy().squeeze() * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "        percentage = np.mean(target_k <= SAMPLING_WEIGHT_THRESHOLD); weights.append(0.1 + percentage)\n",
        "    weights = torch.tensor(weights, dtype=torch.float); torch.save(weights, WEIGHTS_SAVE_PATH)\n",
        "    return weights\n",
        "\n",
        "# --- 3. MAIN TRAINING SCRIPT ---\n",
        "def train_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"--- Starting HiP-EEF Training with DIRECT RESIDUAL PREDICTION on {device} ---\")\n",
        "\n",
        "    train_dataset = MultiVariableARDataset(DATA_DIR, 'train')\n",
        "    val_dataset = MultiVariableARDataset(DATA_DIR, 'val')\n",
        "    sampler = WeightedRandomSampler(get_sampler_weights(train_dataset, train_dataset.stats), len(train_dataset), True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = HiP_EEF_Direct(n_channels=INPUT_CHANNELS).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "    criterion = nn.MSELoss()\n",
        "    print(f\"Loss Weights: Î±={ALPHA} (Cont), Î²={BETA} (Ext), Î´={DELTA} (Final)\")\n",
        "\n",
        "    best_val_loss = float('inf'); patience_counter = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
        "        for predictor, target_norm, _ in pbar:\n",
        "            predictor_subset = predictor[:, VARIABLE_INDICES, :, :].to(device)\n",
        "            target_norm = target_norm.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                final_pred, cont_pred, ext_pred = model(predictor_subset)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    stats = train_dataset.stats\n",
        "                    target_k = target_norm * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "                    background_gt_k, extreme_residual_gt_k = decompose_ground_truth(target_k, RESIDUAL_THRESHOLD_K)\n",
        "                    # Renormalize targets for the model\n",
        "                    background_gt_norm = (background_gt_k - stats['target_mean']) / (stats['target_std'] + 1e-8)\n",
        "                    extreme_residual_gt_norm = extreme_residual_gt_k / (stats['target_std'] + 1e-8)\n",
        "\n",
        "                loss_cont = criterion(cont_pred, background_gt_norm)\n",
        "                loss_ext = criterion(ext_pred, extreme_residual_gt_norm)\n",
        "                loss_final = criterion(final_pred, target_norm)\n",
        "\n",
        "                combined_loss = (ALPHA * loss_cont) + (BETA * loss_ext) + (DELTA * loss_final)\n",
        "\n",
        "            scaler.scale(combined_loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            pbar.set_postfix({'loss': f\"{combined_loss.item():.4f}\"})\n",
        "\n",
        "        # --- Validation Loop ---\n",
        "        model.eval(); val_total_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for predictor, target_norm, _ in val_loader:\n",
        "                predictor_subset = predictor[:, VARIABLE_INDICES, :, :].to(device); target_norm = target_norm.to(device)\n",
        "                final_pred, cont_pred, ext_pred = model(predictor_subset)\n",
        "                stats = val_dataset.stats\n",
        "                target_k = target_norm * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "                background_gt_k, extreme_residual_gt_k = decompose_ground_truth(target_k, RESIDUAL_THRESHOLD_K)\n",
        "                background_gt_norm = (background_gt_k - stats['target_mean']) / (stats['target_std'] + 1e-8)\n",
        "                extreme_residual_gt_norm = extreme_residual_gt_k / (stats['target_std'] + 1e-8)\n",
        "\n",
        "                loss_cont = criterion(cont_pred, background_gt_norm)\n",
        "                loss_ext = criterion(ext_pred, extreme_residual_gt_norm)\n",
        "                loss_final = criterion(final_pred, target_norm)\n",
        "                val_total_loss += ((ALPHA * loss_cont) + (BETA * loss_ext) + (DELTA * loss_final)).item()\n",
        "\n",
        "        avg_val_loss = val_total_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1} | Validation Loss: {avg_val_loss:.6f}\")\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss; torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "            print(f\"   âœ… Best model saved to {MODEL_SAVE_PATH}\"); patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"   ðŸ›‘ Early stopping triggered.\"); break\n",
        "    print(f\"\\n--- ðŸ”¬ Direct Residual Prediction Training Finished. Best Val Loss: {best_val_loss:.6f} ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "5koyegQjLft9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with tiered loss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# --- FINAL TUNED Loss Weights ---\n",
        "# BETA is now the clean \"aggressiveness knob\" for the storm-specialist head.\n",
        "ALPHA = 0.5  # Continuity Head (simple MSE)\n",
        "BETA = 1.5   # Extreme Head (Tiered, aggressive loss)\n",
        "DELTA = 0.4  # Final Fused Prediction (simple MSE)\n",
        "\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_PATH = Path('/content/drive/My Drive/AR_Downscaling')\n",
        "DATA_DIR = PROJECT_PATH / 'final_dataset_multi_variable'\n",
        "OUTPUT_DIR = PROJECT_PATH / 'hip_eef_final_model' # Final model directory\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "MODEL_SAVE_PATH = OUTPUT_DIR / f'hip_eef_final_model_BETA_{BETA}.pth'\n",
        "WEIGHTS_SAVE_PATH = PROJECT_PATH / 'hip_eef_smart_sampling_model' / 'sampler_weights.pt'\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "EPOCHS = 50; BATCH_SIZE = 8; LEARNING_RATE = 1e-4; EARLY_STOPPING_PATIENCE = 10; GRADIENT_CLIP = 1.0\n",
        "\n",
        "# --- HiP-EEF Variable & Loss Configuration ---\n",
        "ALL_VARIABLES = ['IVT', 'T500', 'T850', 'RH700', 'W500']; DL_VARIABLES = ['T500', 'T850', 'RH700', 'W500']\n",
        "DL_INDICES = [ALL_VARIABLES.index(v) for v in DL_VARIABLES]; INPUT_CHANNELS = len(DL_INDICES)\n",
        "\n",
        "\n",
        "# --- Direct Supervision & Tiered Loss Parameters ---\n",
        "RESIDUAL_THRESHOLD_K = 225.0\n",
        "SAMPLING_WEIGHT_THRESHOLD = 220.0\n",
        "LOSS_THRESHOLDS = { 220.0: 10.0, 210.0: 25.0 } # The \"secret sauce\" for the Extreme Head\n",
        "\n",
        "# --- 2. TIERED WEIGHTED LOSS (For Extreme Head ONLY) ---\n",
        "class TieredWeightedMSELoss(nn.Module):\n",
        "    def __init__(self, thresholds: dict):\n",
        "        super().__init__()\n",
        "        self.thresholds = sorted(thresholds.items(), key=lambda item: item[0])\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, prediction_norm, target_residual_norm, original_target_norm, stats):\n",
        "        loss = self.mse(prediction_norm, target_residual_norm)\n",
        "        with torch.no_grad():\n",
        "            # Denormalize the ORIGINAL ground truth to find where the cold pixels are\n",
        "            target_k = original_target_norm * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "            weights = torch.ones_like(target_k)\n",
        "            for temp_k, weight in self.thresholds:\n",
        "                weights[target_k <= temp_k] = weight\n",
        "        return torch.mean(loss * weights)\n",
        "\n",
        "# --- 3. GROUND TRUTH DECOMPOSITION & SAMPLER (Unchanged) ---\n",
        "def decompose_ground_truth(target_k, threshold_k):\n",
        "    background_gt = torch.clamp(target_k, min=threshold_k); extreme_residual_gt = target_k - background_gt\n",
        "    return background_gt, extreme_residual_gt\n",
        "\n",
        "def get_sampler_weights(dataset, stats):\n",
        "    if WEIGHTS_SAVE_PATH.exists():\n",
        "        print(f\"Loading cached sampler weights from {WEIGHTS_SAVE_PATH}\")\n",
        "        return torch.load(WEIGHTS_SAVE_PATH)\n",
        "    # ... (code to calculate weights is the same) ...\n",
        "\n",
        "# --- 4. MAIN TRAINING SCRIPT ---\n",
        "def train_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"--- Starting FINAL HiP-EEF Training with Focused Tiered Loss on {device} ---\")\n",
        "\n",
        "    train_dataset = MultiVariableARDataset(DATA_DIR, 'train')\n",
        "    val_dataset = MultiVariableARDataset(DATA_DIR, 'val')\n",
        "    sampler = WeightedRandomSampler(get_sampler_weights(train_dataset, train_dataset.stats), len(train_dataset), True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = HiP_EEF_Direct(n_channels=INPUT_CHANNELS).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    criterion_mse = nn.MSELoss()\n",
        "    criterion_tiered = TieredWeightedMSELoss(thresholds=LOSS_THRESHOLDS)\n",
        "\n",
        "    print(f\"Using FINAL Focused Loss Weights: Î±={ALPHA}, Î²={BETA}, Î´={DELTA}\")\n",
        "\n",
        "    best_val_loss = float('inf'); patience_counter = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
        "        for predictor, target_norm, _ in pbar:\n",
        "            predictor_subset = predictor[:, DL_INDICES, :, :].to(device); target_norm = target_norm.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                final_pred, cont_pred, ext_pred = model(predictor_subset)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    stats = train_dataset.stats\n",
        "                    target_k = target_norm * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "                    background_gt_k, extreme_residual_gt_k = decompose_ground_truth(target_k, RESIDUAL_THRESHOLD_K)\n",
        "                    background_gt_norm = (background_gt_k - stats['target_mean']) / (stats['target_std'] + 1e-8)\n",
        "                    extreme_residual_gt_norm = extreme_residual_gt_k / (stats['target_std'] + 1e-8)\n",
        "\n",
        "                # --- APPLY THE CORRECT, FOCUSED LOSS TO EACH HEAD ---\n",
        "                loss_cont = criterion_mse(cont_pred, background_gt_norm)\n",
        "                loss_ext = criterion_tiered(ext_pred, extreme_residual_gt_norm, target_norm, train_dataset.stats)\n",
        "                loss_final = criterion_mse(final_pred, target_norm)\n",
        "\n",
        "                combined_loss = (ALPHA * loss_cont) + (BETA * loss_ext) + (DELTA * loss_final)\n",
        "\n",
        "            scaler.scale(combined_loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            pbar.set_postfix({'loss': f\"{combined_loss.item():.4f}\"})\n",
        "\n",
        "        # --- Validation Loop ---\n",
        "        model.eval(); val_total_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for predictor, target_norm, _ in val_loader:\n",
        "                # ... (validation logic follows the same focused loss structure) ...\n",
        "                predictor_subset = predictor[:, DL_INDICES, :, :].to(device); target_norm = target_norm.to(device)\n",
        "                final_pred, cont_pred, ext_pred = model(predictor_subset)\n",
        "                stats = val_dataset.stats\n",
        "                target_k = target_norm * (stats['target_std'] + 1e-8) + stats['target_mean']\n",
        "                background_gt_k, extreme_residual_gt_k = decompose_ground_truth(target_k, RESIDUAL_THRESHOLD_K)\n",
        "                background_gt_norm = (background_gt_k - stats['target_mean']) / (stats['target_std'] + 1e-8)\n",
        "                extreme_residual_gt_norm = extreme_residual_gt_k / (stats['target_std'] + 1e-8)\n",
        "\n",
        "                loss_cont = criterion_mse(cont_pred, background_gt_norm)\n",
        "                loss_ext = criterion_tiered(ext_pred, extreme_residual_gt_norm, target_norm, val_dataset.stats)\n",
        "                loss_final = criterion_mse(final_pred, target_norm)\n",
        "                val_total_loss += ((ALPHA * loss_cont) + (BETA * loss_ext) + (DELTA * loss_final)).item()\n",
        "\n",
        "        avg_val_loss = val_total_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1} | Validation Loss: {avg_val_loss:.6f}\")\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss; torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "            print(f\"   âœ… Best model saved to {MODEL_SAVE_PATH}\"); patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"   ðŸ›‘ Early stopping triggered.\"); break\n",
        "    print(f\"\\n--- ðŸ”¬ FINAL Focused Loss Training Finished. Best Val Loss: {best_val_loss:.6f} ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_model()"
      ],
      "metadata": {
        "id": "m6XgquVIuMMm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}